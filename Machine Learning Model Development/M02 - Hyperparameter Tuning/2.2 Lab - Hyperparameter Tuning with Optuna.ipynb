{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a44f1999-d211-4ecb-8b5c-a687e855cbb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9f8cb8-a54b-4705-9c5f-96495ff8b2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB - Hyperparameter Tuning with Optuna\n",
    "\n",
    "Welcome to the Hyperparameter Tuning with Optuna lab! In this hands-on session, you'll gain practical insights into **optimizing machine learning models using Optuna**. Throughout the lab, we'll cover key steps, from loading the dataset and creating training/test sets to **defining a hyperparameter search space and running optimization trials with Spark**. The primary objective is to equip you with the skills to fine-tune models effectively using Spark, Optuna, and MLflow.\n",
    "\n",
    "**Lab Outline:**\n",
    "1. Load the dataset and create training/test sets for a scikit-learn model. \n",
    "1. Define the hyperparameter search space for optimization.\n",
    "1. Define the optimization function to fine-tune the model.\n",
    "1. Run hyperparameter tuning trials. \n",
    "1. Search for runs using the MLflow API and visualize all runs within the MLflow experiment.\n",
    "1. Identify the best run based on the model's precision value programmatically and visually.\n",
    "1. Register the model with Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb69131c-04d2-4e00-be79-2cd5f459d894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb2a8ae-40c3-4945-8ac0-3454b3728bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bcfc648-13ca-43ac-9ba1-de073db35241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the lab, run the provided classroom setup script. This script will define configuration variables necessary for the lab. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "433bc4ab-97a6-447d-a7f5-e80307058822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qq optuna\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2ca569-1bd2-4069-8b26-e64a679fac71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d845895-5af3-4490-948d-9bcb5b73ff67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66bd4974-f93d-4141-95e1-e133b1e67137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7323705b-5789-4d69-9956-2bba7728e2e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "In this lab, you will be using a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including gender, as well as internet subscription details such as subscription plans and payment methods.\n",
    "\n",
    "In this lab, we will create and tune a model that will predict customer churn based on the **`Churn`** field. \n",
    "\n",
    "A table with all features is already created for you.\n",
    "\n",
    "**Table name: `customer_churn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48d28dc2-4d94-4464-b2a3-e8e260ae4a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "## load the table from Unity Catalog called custome_churn\n",
    "table_name = <FILL_IN>\n",
    "## Read into a PySpark DataFrame and convert to Pandas DataFrame\n",
    "diabetes_dataset = <FILL_IN>\n",
    "customer_pd = <FILL_IN>\n",
    "\n",
    "## split dataset between features and targets. The target variable is Churn\n",
    "target_col = <FILL_IN>\n",
    "X_all = <FILL_IN>\n",
    "y_all = <FILL_IN>\n",
    "\n",
    "## test / train split using 95% train/5% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(<FILL_IN>)\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7293cf5e-b4b2-4637-a122-41c1e0e31c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "## load the table from Unity Catalog called custome_churn\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.customer_churn\"\n",
    "## Read into a PySpark DataFrame and convert to Pandas DataFrame\n",
    "diabetes_dataset = spark.read.table(table_name)\n",
    "customer_pd = diabetes_dataset.drop('CustomerID').toPandas()\n",
    "\n",
    "## split dataset between features and targets. The target variable is Churn\n",
    "target_col = \"Churn\"\n",
    "X_all = customer_pd.drop(labels=target_col, axis=1)\n",
    "y_all = customer_pd[target_col]\n",
    "\n",
    "## test / train split using 95% train/5% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85fe1ee1-455b-49d8-b17c-64dbef4c9a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Define the Search Space and Optimization Function\n",
    "\n",
    "Define the parameter search space for Optuna.\n",
    "\n",
    "Your objective function should meet the following requirements:\n",
    "\n",
    "1. Define the search space using the hyperparameters `max_depth` and `max_features`. For `max_depth`, the search range should be between 5 and 50, while `max_features` should be between 5 and 10. Additionally, for the `criterion` parameter, search based on `gini`, `entropy`, and `log_loss`. \n",
    "1. Enable MLflow run as a nested experiment.\n",
    "1. For each run, log the cross-validation results for `accuracy`, `precision`, `recall`, and `f1`.\n",
    "1. Use **3-fold** cross-validation. Be sure to average the fold results using `.mean()`.\n",
    "1. The objective will be to _maximize_ **`precision`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2bd57e8-472d-4ea3-b6cb-a66407df30e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "## Define the objective function\n",
    "def optuna_objective_function(<FILL_IN>):\n",
    "    params = {\n",
    "        'criterion': <FILL_IN>,\n",
    "        'max_depth': <FILL_IN>,\n",
    "        'max_features': <FILL_IN>\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(nested=True, run_name=f\"Optuna Trial {trial.number}\"):\n",
    "        \n",
    "        ## Train model\n",
    "        dtc = <FILL_IN>\n",
    "\n",
    "        ## Perform cross-validation\n",
    "        scoring_metrics = [<FILL_IN>]\n",
    "        cv_results = cross_validate(<FILL_IN>)\n",
    "\n",
    "        ## Create input signature using the first row of X_train\n",
    "        input_example = X_train.iloc[[0]]\n",
    "        signature = <FILL_IN>\n",
    "\n",
    "        ## Compute and log average scores\n",
    "        cv_results_avg = {metric: cv_results[f'test_{metric}'].mean() for metric in scoring_metrics}\n",
    "        mlflow.log_metrics(<FILL_IN>)\n",
    "        mlflow.log_params(<FILL_IN>)\n",
    "        mlflow.sklearn.log_model(<FILL_IN>)\n",
    "\n",
    "        ## Return precision to maximize it\n",
    "        return <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cb72c64-72a4-4524-9aed-5c475c46aae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "## Define the objective function\n",
    "def optuna_objective_function(trial):\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'max_features': trial.suggest_int('max_features', 5, 10)\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(nested=True, run_name=f\"Optuna Trial {trial.number}\"):\n",
    "        \n",
    "        ## Train model\n",
    "        dtc = DecisionTreeClassifier(**params)\n",
    "        dtc.fit(X_train, y_train)\n",
    "\n",
    "        ## Perform cross-validation\n",
    "        scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        cv_results = cross_validate(dtc, X_train, y_train, cv=3, scoring=scoring_metrics)\n",
    "\n",
    "        ## Create input signature using the first row of X_train\n",
    "        input_example = X_train.iloc[[0]]\n",
    "        signature = infer_signature(input_example, dtc.predict(input_example))\n",
    "\n",
    "        ## Compute and log average scores\n",
    "        cv_results_avg = {metric: cv_results[f'test_{metric}'].mean() for metric in scoring_metrics}\n",
    "        mlflow.log_metrics(cv_results_avg)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.sklearn.log_model(dtc, \"lab_optuna_decision_tree_model\", signature = signature, input_example=input_example)\n",
    "\n",
    "        ## Return precision to maximize it\n",
    "        return cv_results_avg['precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e97ea11-1d8a-4631-ba43-824a1dae8a98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create an Optuna Study and Log with MLflow\n",
    "\n",
    "First, we will delete all previous runs to keep our workspace and experiment tidy. Second, you will create an Optuna study and run the experiment with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f801ddad-8d22-4adc-b9b7-c49e7962d809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Set the MLflow experiment name and get the id\n",
    "experiment_name = f\"/Users/{DA.username}/Lab_Optuna_Experiment_{DA.schema_name}\"\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "print(\"Clearing out old runs (If you want to add more runs, change the n_trial parameter in the next cell) ...\")\n",
    "## Get all runs\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id], output_format=\"pandas\")\n",
    "\n",
    "if runs.empty:\n",
    "    print(\"No runs found in the experiment.\")\n",
    "else:\n",
    "    ## Iterate and delete each run\n",
    "    for run_id in runs[\"run_id\"]:\n",
    "        mlflow.delete_run(run_id)\n",
    "        print(f\"Deleted run: {run_id}\")\n",
    "\n",
    "    print(\"All runs have been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "712cda8d-fce6-4540-ace7-08f3a44eca69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create the Study and Log with MLflow\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. Create an Optuna study with name `lab_optuna_hpo`.\n",
    "1. Maximize the objective function. \n",
    "1. Give the parent run the name `Lab_Optuna_Hyperparameter_Optimization`.\n",
    "1. Only run 10 trials with Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf467f7-6b7d-4ba1-afaa-2247c6fa4ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = <FILL_IN>\n",
    "\n",
    "with mlflow.start_run(run_name='Lab_Optuna_Hyperparameter_Optimization') as parent_run:\n",
    "    <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "917da71f-f01e-4699-be58-d369bd34eb76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "study = optuna.create_study(\n",
    "    study_name=\"lab_optuna_hpo\",\n",
    "    direction=\"maximize\"\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name='Lab_Optuna_Hyperparameter_Optimization') as parent_run:\n",
    "    ## Run optimization\n",
    "    study.optimize(\n",
    "        optuna_objective_function, \n",
    "        n_trials=10,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3a63c05-2804-4394-b2aa-97039bd6307e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3. Visual Inspection of Precision Values\n",
    "\n",
    "Here, we can view all 10 runs. After completing the code and running the following cell, scroll to the right and locate the column `metrics.precision`. Use the UI to order and order by descending. This will locate the largest precision score. Next, you will create a visual to also help understand the distribution of scores by trial. \n",
    "\n",
    "\n",
    "### Creating a precision score visual\n",
    "\n",
    "1. **Run the next cell** to generate the table output.  \n",
    "1. Click on the **plus (+) symbol** in the output cell.  \n",
    "1. Select **Visualization** from the options.  \n",
    "1. In the visualization settings, choose \n",
    "**Bar** and ensure **Horizontal Chart** toggle is **on**.  \n",
    "1. Configure the **Y-axis**:  \n",
    "   - Set **Y Column** to `tags.mlflow.runName`.  \n",
    "1. Configure the **X-axis**:  \n",
    "   - Set **X Columns** to `metrics.precision`.  \n",
    "   - Choose **Sum** as the aggregation method.  \n",
    "1. Click on the **Y-axis tab**:  \n",
    "   - Ensure **Show Labels** is **on**.  \n",
    "1. Apply the settings and visualize the data.\n",
    "\n",
    "\n",
    "After following the above instructions, visually inspect which trial had the best run according to `precision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e09e8d36-21ae-453e-bd82-d1cdb8cec4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "## Define your experiment name or ID\n",
    "experiment_id = parent_fun.experiment_id\n",
    "\n",
    "## Fetch all runs from the experiment using the MLflow API\n",
    "df_runs = mlflow.search_runs(<FILL_IN>)\n",
    "\n",
    "display(df_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a8b3d2-2d4e-4b25-8d05-880e61dd0c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "## Define your experiment name or ID\n",
    "experiment_id = parent_run.info.experiment_id # Replace with your actual experiment ID\n",
    "\n",
    "## Fetch all runs from the experiment\n",
    "df_runs = mlflow.search_runs(\n",
    "  experiment_ids=[experiment_id]\n",
    "  )\n",
    "\n",
    "display(df_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ad28a73-b489-4f89-b47d-ca6779a98f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4. Find the Best Run Programmatically\n",
    "\n",
    "In this step you will find the best scores using the Optuna library to find the best value and parameter values. Additionally, you will use MLflow to find these values. \n",
    "\n",
    "#### Instructions\n",
    "1. Use the Optuna study to find the best precision score. \n",
    "1. Use the Optuna study to find the best hyperparameter values. \n",
    "1. Use the MLflow API to find the best run based on precision score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9150374f-e6ad-4de3-abd5-12a0f5ee8772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Display the best hyperparameters and metric\n",
    "print(f\"Best hyperparameters: {<FILL_IN>}\")\n",
    "print(f\"Best precision score: {<FILL_IN>}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c09936dd-1730-4b3e-8f19-fede5296e295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Display the best hyperparameters and metric\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best precision score: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f601ba6e-4ca4-46dd-b706-09f27945ce5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_runs_pd = (mlflow.search_runs(<FILL_IN>))\n",
    "\n",
    "## convert search_runs_pd to pyspark dataframe\n",
    "search_runs_sd = <FILL_IN>\n",
    "display(search_runs_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff06a421-85c5-4ba5-81ba-e558698763bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "search_runs_pd = (mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"metrics.precision DESC\"],\n",
    "    max_results=1))\n",
    "\n",
    "## convert search_runs_pd to pyspark dataframe\n",
    "search_runs_sd = spark.createDataFrame(search_runs_pd)\n",
    "display(search_runs_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4a2475b-4184-46d2-8721-839a3ed806cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the Best Model and Parameters and Register to Unity Catalog\n",
    "\n",
    "#### Instructions:\n",
    "1. Either use the results from above to copy and paste the run_id and experiment_id below or perform this task programmatically using `.collect()` on the `search_runs` PySpark DataFrame. \n",
    "1. Load the model from MLflow.\n",
    "1. Display the results for the best model and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5aa5e13-68ac-487a-b689-fca2fcc25d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Get the string value from run_id and experiment_id from PySpark DataFrame hpo_runs_df\n",
    "run_id = <FILL_IN>\n",
    "experiment_id = <FILL_IN>\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cade05f9-6a04-4b9d-9598-42e4b7e11908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Get the string value from run_id and experiment_id from PySpark DataFrame hpo_runs_df\n",
    "run_id = search_runs_sd.select(\"run_id\").collect()[0][0]\n",
    "experiment_id = search_runs_sd.select(\"experiment_id\").collect()[0][0]\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57e68ab-a794-4d1a-a8d9-e63938a7bd1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "from mlflow.models import Model\n",
    "\n",
    "Grab an input example from the test set (pandas DataFrame)\n",
    "input_example = X_test.iloc[[0]]\n",
    "\n",
    "You logged the model as: mlflow.sklearn.log_model(dtc, \"lab_optuna_decision_tree_model\", ...)\n",
    "model_uri = <FILL_IN>\n",
    "\n",
    "## Load the model\n",
    "loaded_model = <FILL_IN>\n",
    "\n",
    "## Retrieve model parameters MLflow client and get_run() method\n",
    "client = <FILL_IN>\n",
    "params = <FILL_IN>\n",
    "\n",
    "## Display model parameters\n",
    "print(\"Best Model Parameters:\")\n",
    "print(json.dumps(params, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4d3411-8736-43f0-b10c-37599f7d74ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import mlflow\n",
    "import json\n",
    "\n",
    "# Grab an input example from the test set (pandas DataFrame)\n",
    "input_example = X_test.iloc[[0]]\n",
    "\n",
    "# You logged the model as: mlflow.sklearn.log_model(dtc, \"lab_optuna_decision_tree_model\", ...)\n",
    "model_uri = f\"runs:/{run_id}/lab_optuna_decision_tree_model\"\n",
    "\n",
    "# Load the model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Retrieve model parameters via MLflow client\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "params = client.get_run(run_id).data.params\n",
    "\n",
    "# Display model parameters\n",
    "print(\"Best Model Parameters:\")\n",
    "print(json.dumps(params, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5599c3e6-6bd7-4a86-9096-1392aa5ec4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Register the Model to Unity Catalog\n",
    "\n",
    "Register your model to Unity Catalog under the name `lab_optuna_model`. \n",
    "\n",
    "> _You can get the catalog name and schema name using `DA.catalog_name` and `DA.schema_name`, respectively._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc761364-2338-4c9b-8450-0f6fafb6d943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_uri = <FILL_IN>\n",
    "mlflow.register_model(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c2aa015-6611-4bf7-8382-68c7b30de0f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_uri = f'runs:/{run_id}/lab_optuna_decision_tree_model'\n",
    "mlflow.register_model(model_uri=model_uri, name=f\"{DA.catalog_name}.{DA.schema_name}.lab_optuna_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81e9eb3d-3774-4206-80d5-d261509f4556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab, you learned about Optuna and how to integrate Optuna trials and studies with MLflow. You also demonstrated the ability to programmatically and visually inspect the best trial. Finally, you showed how to load the MLflow model and register it to Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c64add-c2c5-430f-b524-3e525e82c69d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.2 Lab - Hyperparameter Tuning with Optuna",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}