{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e18bdcf-889a-410b-a426-3fb1f5d28c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58464e30-13fb-4de0-a449-1df99889f665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Hyperparameter Tuning with Optuna\n",
    "\n",
    "In this hands-on demo, you will learn how to leverage **Optuna**, a powerful optimization library, for efficient model tuning. We'll guide you through the process of performing **hyperparameter optimization**, demonstrating how to define the search space, objective function, and algorithm selection. Throughout the demo, you will utilize *MLflow* to seamlessly track the model tuning process, capturing essential information such as hyperparameters, metrics, and intermediate results. By the end of the session, you will not only grasp the principles of hyperparameter optimization but also be proficient in finding the best-tuned model using various methods such as the **MLflow API** and **MLflow UI**.\n",
    "\n",
    "By integrating Optuna and MLflow, you can efficiently optimize hyperparameters and maintain comprehensive records of your machine learning experiments, facilitating reproducibility and collaborative research.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "- Perform hyperparameter optimization using Optuna.\n",
    "- Track the model tuning process with MLflow.\n",
    "- Query previous runs from an experiment using the `MLflowClient`.\n",
    "- Review an MLflow Experiment for visualizing results and selecting the best run.\n",
    "- Read in the best model, make a prediction, and register the model to Unity Catalog. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d91a8a4-8c46-45d0-9774-3305ff73bb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8177dd8c-d1f0-4b4c-bd58-0882d7103294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b571f1a3-0dd5-439a-b2a8-1a233f2cd995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec0d044-25f0-4dfb-bd5f-cdbbf295102b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qq optuna\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90cc6f4-85aa-426c-8745-3006b29bb950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6d48e15-0724-43c3-a871-940593b490f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7310cc37-1329-4b11-aef6-747c9a2793f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fafc1f11-2aa5-4563-b502-d0ae2c09f9da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Before we start fitting a model, we need to prepare dataset. First, we will load dataset, then we will split it to train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d29dfefe-8e6e-4c1d-a668-0be38c757d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "In this demo we will be using the CDC Diabetes dataset from the Databricks Marketplace. This dataset has been read in and written to a feature table called `diabetes` in our working schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18ef74e4-b098-4025-80fc-ff64533c0b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load data from the feature table\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.diabetes\"\n",
    "diabetes_dataset = spark.read.table(table_name)\n",
    "diabetes_pd = diabetes_dataset.drop('unique_id').toPandas()\n",
    "\n",
    "# review dataset and schema\n",
    "display(diabetes_pd)\n",
    "print(diabetes_pd.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ca1631c-d135-40e1-83a0-b602b6a0b1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Train/Test Split\n",
    "\n",
    "Next, we will divide the dataset to training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6efb76c-5593-49f3-9b6f-4d25516135dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"We have {diabetes_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into it's own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "X_all = diabetes_pd.drop(labels=target_col, axis=1)\n",
    "y_all = diabetes_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e04e235-9da6-423f-90a7-99ad1403db88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e9671b4-f9a4-41cf-8642-3133117dd474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the Objective Function\n",
    "\n",
    "An objective function in Optuna is a Python function that defines the optimization target. It takes a single argument, typically named trial, which is an instance of the optuna.Trial class. This function is responsible for:\n",
    "\n",
    "1. Defining the hyperparameter search space\n",
    "\n",
    "1. Training the model with the suggested hyperparameters\n",
    "\n",
    "1. Evaluating the model's performance\n",
    "\n",
    "1. Returning a scalar value that Optuna will try to optimize (minimize or maximize)\n",
    "\n",
    "In our case, we are working with scikit-learn's `DecisionTreeClassifier`. Start by defining the search space for the model. Our hyperparameters are:\n",
    "- `criterion`: chooses between `gini` and `entropy`. Defining the criterion parameter allows the algorithm to try both options during tuning and can assist in identifying which criterion works best. [TPE](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html#optuna.samplers.TPESampler) is the [default](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html#pruning), though there are [other sampling methods](https://optuna.readthedocs.io/en/stable/reference/samplers/index.html) like GPSampler and BruteForceSampler.\n",
    "- `max_depth`: an integer between 5 and 50\n",
    "- `min_samples_split`: an integer between 2 and 40\n",
    "- `min_samples_leaf`: an integer between 1 and 20\n",
    "\n",
    "The objective function will also have a nested MLflow runs for logging each trial start a new MLflow run for each trial using `with mlflow.start_run()`. We will also manually log metrics and the scikit-learn model within the objective function. Note that the training process is using cross-validation (5-fold CV in fact) and returns the negative mean of the fold results. \n",
    "\n",
    "\n",
    ">   - _**Gini impurity** measures how often a randomly chosen sample would be incorrectly classified if randomly labeled according to the current class distribution. It quantifies the probability of misclassification._\n",
    ">  - _**Entropy** measures the amount of uncertainty or disorder in the dataset. It quantifies how “impure” a node is in terms of class distribution, with higher entropy meaning more disorder (i.e., more uncertainty in classification)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15fda790-c27b-4ecb-90a1-a68820d011e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Define the objective function\n",
    "def optuna_objective_function(trial):\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 40),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    }\n",
    "    \n",
    "    # Start an MLflow run for logging\n",
    "    with mlflow.start_run(nested=True, run_name=f\"Model Tuning with Optuna - Trial {trial.number}\"):\n",
    "\n",
    "        # Log parameters with MLflow\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        dtc = DecisionTreeClassifier(**params)\n",
    "        scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        cv_results = cross_validate(dtc, X_train, y_train, cv=5, scoring=scoring_metrics, return_estimator=True)\n",
    "        \n",
    "        # Log cross-validation metrics to MLflow\n",
    "        for metric in scoring_metrics:\n",
    "            mlflow.log_metric(f'cv_{metric}', cv_results[f'test_{metric}'].mean())\n",
    "\n",
    "        # Train the model on the full training set\n",
    "        final_model = DecisionTreeClassifier(**params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "        # Create input signature using the first row of X_train\n",
    "        input_example = X_train.iloc[[0]]\n",
    "        signature = infer_signature(input_example, final_model.predict(input_example))\n",
    "\n",
    "        # Log the model with input signature\n",
    "        mlflow.sklearn.log_model(final_model, \"decision_tree_model\", signature=signature, input_example=input_example)\n",
    "\n",
    "        # Compute the mean from cross-validation\n",
    "        f1_score_mean = cv_results['test_f1'].mean()\n",
    "\n",
    "        # Metric to be minimized\n",
    "        return -f1_score_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ad00b2-651e-45ec-8f4f-9eae79c21b0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimize the Scikit-Learn Model on Single-Machine Optuna and Log Results with MLflow\n",
    "\n",
    "Before running the optimization, we need to perform two key steps:\n",
    "\n",
    "1. **Initialize an Optuna Study** using `optuna.create_study()`.  \n",
    "   - A *study* represents an optimization process consisting of multiple trials.  \n",
    "   - A *trial* is a single execution of the *objective function* with a specific set of hyperparameters.  \n",
    "\n",
    "2. **Run the Optimization** using `study.optimize()`.  \n",
    "   - This tells Optuna how many trials to perform and allows it to explore the search space.  \n",
    "\n",
    "Each trial will be logged to MLflow, including the hyperparameters tested and their corresponding cross-validation results. Optuna will handle the optimization while training continues.\n",
    "\n",
    "#### **Steps:**\n",
    "- **Set up an Optuna study** with `optuna.create_study()`. \n",
    "- **Start an MLflow run** with `mlflow.start_run()` to log experiments. \n",
    "- **Optimize hyperparameters** using `study.optimize()` within the MLflow context.\n",
    "\n",
    "> **Note on `n_jobs` in `study.optimize()`:**  \n",
    "> The `n_jobs` argument controls the **number of trials running in parallel** using multi-threading **on a single machine**.  \n",
    "> - If `n_jobs=-1`, Optuna will use **all available CPU cores** (e.g., on a 4-core machine, it will likely use all 4 cores).  \n",
    "> - If `n_jobs` is **undefined (default)**, trials run **sequentially (single-threaded)**.  \n",
    "> - **Important:** `n_jobs` does **not** distribute trials across multiple nodes in a Spark cluster. To parallelize across nodes, use `SparkTrials()` instead.\n",
    "\n",
    "> **Why We Don't Use `MLflowCallback`:**  \n",
    "> Optuna provides an [`MLflowCallback`](https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.integration.MLflowCallback.html) for automatic logging. However, in this demo, we are demonstrating how to integrate the MLflow API with Optuna separate from `MLflowCallback`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92132053-309d-4c04-8b4d-f049ce34b28b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, we will delete all previous runs to keep our workspace and experiment tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df22db63-18b8-4e25-9118-70487312383a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the MLflow experiment name and get the id\n",
    "experiment_name = f\"/Users/{DA.username}/Hyperparameter_Tuning_with_Optuna_{DA.schema_name}\"\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "print(\"Clearing out old runs (If you want to add more runs, change the n_trial parameter in the next cell) ...\")\n",
    "# Get all runs\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id], output_format=\"pandas\")\n",
    "\n",
    "if runs.empty:\n",
    "    print(\"No runs found in the experiment.\")\n",
    "else:\n",
    "    # Iterate and delete each run\n",
    "    for run_id in runs[\"run_id\"]:\n",
    "        mlflow.delete_run(run_id)\n",
    "        print(f\"Deleted run: {run_id}\")\n",
    "\n",
    "    print(\"All runs have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a91ff5ab-6d49-4964-ba3a-6dfd30be58af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"optuna_hpo\",\n",
    "    direction=\"minimize\"\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name='demo_optuna_hpo') as parent_run:\n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        optuna_objective_function, \n",
    "        n_trials=10\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2ff8312-326a-486b-b5dd-22b3f2125214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Review Tuning Results\n",
    "\n",
    "We can use the MLflow API to review the trial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f91975-0a92-4827-a003-fb5e272d80d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Define your experiment name or ID\n",
    "experiment_id = parent_run.info.experiment_id # Replace with your actual experiment ID\n",
    "\n",
    "# Fetch all runs from the experiment\n",
    "df_runs = mlflow.search_runs(\n",
    "  experiment_ids=[experiment_id]\n",
    "  )\n",
    "\n",
    "df_runs = df_runs[df_runs['tags.mlflow.runName'] != 'demo_optuna_hpo']\n",
    "\n",
    "display(df_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b411a7-aaf3-402f-8f2a-7c21f8fb2125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can use the Optuna study to get the best parameters and F1-score. Validate this agrees with the table results from the previous cell's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c7129be-2393-42ad-9005-dfa9e9ed7881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the best hyperparameters and metric\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best negative-F1 score: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6b9a01a-f54f-435a-b54a-e4c8e4271f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Find the Best Run Based on F1-Score\n",
    "\n",
    "In this section, we will search for registered models. There are couple of ways to achieve this. We will show how to search runs using MLflow API and the UI.\n",
    "\n",
    "**The output links for using Optuna gave the best runs. Why can't we just use that?**\n",
    "\n",
    "You totally can! But this is the same as using the UI to navigate to the trial that was the best (which is shown below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c47d33a5-18b6-4b9f-801d-751b068d839c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 1: Find the Best Run - MLFlow API\n",
    "\n",
    "Using the MLFlow API, you can search runs in an experiment, which returns results into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "234cecae-95bc-42d2-add6-ae540615bc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_id = parent_run.info.experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6252368-bd18-4286-a6f6-0383fda46d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "search_runs_pd = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"metrics.cv_f1 DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "display(search_runs_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a74871a2-28fb-4687-8e37-e328c81b0bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "search_runs_pd = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"metrics.cv_f1 ASC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "display(search_runs_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b665c6b1-bb2c-4427-a42c-63c669405988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 2 - Find the Best Run - MLflow UI\n",
    "\n",
    "The simplest way of seeing the tuning result is to use MLflow UI. \n",
    "\n",
    "1. Click on **Experiments** from left menu.\n",
    "\n",
    "1. Select experiment which has the same name as this notebook's title (**`Hyperparameter_Tuning_with_Optuna_{schema_name}`**).\n",
    "\n",
    "1. Click on the graph icon at the top left under **Runs**.\n",
    "\n",
    "1. Click on the parent run or manually select all 10 runs to compare. The graphs on the right of the screen will appear for inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2ee6b14-a8ee-46ba-9077-3f2adcc55cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Visualize the Hyperparameters \n",
    "\n",
    "By now, we have determined which trial had the best run according to the f1-score. Now, let's visually inspect our other search space elements with respect to this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8aad382-3465-4bc4-b824-0595162bbafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Ensure the necessary parameters exist in the DataFrame before plotting\n",
    "required_params = [\"params.min_samples_leaf\", \"params.max_depth\", \"params.min_samples_split\", \"metrics.cv_f1\", \"tags.mlflow.runName\"]\n",
    "df_filtered = df_runs.dropna(subset=required_params, how=\"any\")\n",
    "\n",
    "# Convert parameters to appropriate types\n",
    "df_filtered[\"params.min_samples_split\"] = df_filtered[\"params.min_samples_split\"].astype(float)\n",
    "df_filtered[\"params.max_depth\"] = df_filtered[\"params.max_depth\"].astype(float)\n",
    "df_filtered[\"metrics.cv_f1\"] = df_filtered[\"metrics.cv_f1\"].astype(float)\n",
    "\n",
    "# Identify the best run index (assuming higher f1 is better)\n",
    "best_run_index = df_filtered[\"metrics.cv_f1\"].idxmax()\n",
    "best_run_name = df_filtered.loc[best_run_index, \"tags.mlflow.runName\"]\n",
    "\n",
    "# Extract run names for x-axis labels\n",
    "run_names = df_filtered[\"tags.mlflow.runName\"]\n",
    "\n",
    "# Create a figure and axis for bar chart\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bar chart for min_samples_split and max_depth\n",
    "df_filtered[[\"params.min_samples_split\", \"params.max_depth\"]].plot(kind=\"bar\", ax=ax1, edgecolor=\"black\")\n",
    "\n",
    "ax1.set_xlabel(\"Run Name\")\n",
    "ax1.set_ylabel(\"Parameter Values\")\n",
    "ax1.set_title(\"Hyperparameters & cv_f1 Score per Run\")\n",
    "ax1.legend([\"Max Features\", \"Max Depth\"])\n",
    "ax1.set_xticks(range(len(df_filtered)))\n",
    "ax1.set_xticklabels(run_names, rotation=45, ha=\"right\")  # Rotate for readability\n",
    "\n",
    "# Create a second y-axis for the cv_f1 score line chart\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    range(len(df_filtered)),  # X-axis indices\n",
    "    df_filtered[\"metrics.cv_f1\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"blue\",\n",
    "    label=\"cv_f1 Score\"\n",
    ")\n",
    "\n",
    "# Highlight the best run with a bold marker\n",
    "ax2.plot(\n",
    "    df_filtered.index.get_loc(best_run_index),  # Get positional index\n",
    "    df_filtered.loc[best_run_index, \"metrics.cv_f1\"],\n",
    "    marker=\"o\",\n",
    "    markersize=10,\n",
    "    color=\"red\",\n",
    "    label=\"Best Run\"\n",
    ")\n",
    "\n",
    "# Add a vertical dashed line to indicate the best run\n",
    "ax2.axvline(df_filtered.index.get_loc(best_run_index), color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax2.set_ylabel(\"cv_f1 Score\")\n",
    "\n",
    "# Add legend\n",
    "fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "plt.show()\n",
    "\n",
    "# Pie chart for criterion\n",
    "plt.figure(figsize=(8, 8))\n",
    "df_filtered[\"params.criterion\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\", startangle=90)\n",
    "plt.title(\"Criterion Distribution\")\n",
    "plt.ylabel(\"\")  # Hide y-label for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0e9728c-3a9f-46d0-b7b5-9655af2d5d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the Best Model and Parameters\n",
    "To load the model and make a prediction, let's use the information from Option 2 shown above. Run the next cell to get the value.\n",
    "\n",
    "\n",
    "### Copy and Paste Option\n",
    "Alternatively, you can set the variables shown below manually. Using either the output from Option 1 or Option 2 or the UI from Option 3, locate the `run_id` and the `experiment_id`. With Option 1 or 2, this is simply the value in the first two columns. In the UI, this is presented to you in the Details table when clicking on the specific run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14506ecf-c241-41f1-9432-3071178b14f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert search_runs_pd to pyspark dataframe\n",
    "search_runs_sd = spark.createDataFrame(search_runs_pd)\n",
    "\n",
    "\n",
    "# Get the string value from run_id and experiment_id from PySpark DataFrame hpo_runs_df\n",
    "run_id = search_runs_sd.select(\"run_id\").collect()[0][0]\n",
    "experiment_id = search_runs_sd.select(\"experiment_id\").collect()[0][0]\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74d613a3-74f8-4473-a2be-cc78e9dcc027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "\n",
    "# Grab an input example from the test set (pandas DataFrame)\n",
    "input_example = X_test.iloc[[0]]\n",
    "\n",
    "# (works regardless of whether artifacts are stored on DBFS/S3/UC-managed storage)\n",
    "model_uri = f\"runs:/{run_id}/decision_tree_model\"\n",
    "\n",
    "# Load the model using the run URI\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Retrieve model parameters\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "params = client.get_run(run_id).data.params\n",
    "\n",
    "# Display model parameters\n",
    "print(\"Best Model Parameters:\")\n",
    "print(json.dumps(params, indent=4))\n",
    "\n",
    "# (Optional) sanity check: run a prediction\n",
    "pred = loaded_model.predict(input_example)\n",
    "print(\"Prediction:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "176833c2-3dfe-4c2b-9266-f1006eb593c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04c6bfd4-a9b0-4640-b75d-9d6214c8fea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "test_prediction = loaded_model.predict(input_example)\n",
    "# X_test is a pandas dataframe - let's add the test_prediction output as a new column\n",
    "input_example['prediction'] = test_prediction\n",
    "display(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b085d8ad-a5f0-4778-acbc-25f13f516604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Register the Model to Unity Catalog\n",
    "\n",
    "After running the following cell, navigate to our working catalog and schema (see course setup above) and validate the model has been registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57adc979-064b-49d8-9bb4-3c00d1ffdf16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_uri = f'runs:/{run_id}/decision_tree_model'\n",
    "mlflow.register_model(model_uri=model_uri, name=f\"{DA.catalog_name}.{DA.schema_name}.demo_optuna_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aedcb279-3fe4-4fcb-9948-8e4c625e0276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this demo, we've explored how to enhance your model's performance using **Optuna** for hyperparameter optimization and **MLflow** for tracking the tuning process. By employing Optuna's efficient search algorithms, you've learned to fine-tune your model's parameters effectively. Simultaneously, MLflow has facilitated seamless monitoring and logging of each trial, capturing essential information such as hyperparameters, metrics, and intermediate results. Additionally, you learned how to register the best model within Unity Catalog. Moving forward, integrating these tools into your workflow will be instrumental in improving your model's performance and simplifying the fine-tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd9b5636-580e-4637-b634-6f9bc5608891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.1 Demo - Hyperparameter Tuning with Optuna",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}