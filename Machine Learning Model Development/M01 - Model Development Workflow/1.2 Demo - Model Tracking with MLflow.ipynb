{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "101e6be1-5c17-4506-abef-bffde23b4448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f561e9e-0ebf-4eb5-abe6-feb2393ac8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Model Tracking with *MLflow*\n",
    "\n",
    "In this demo, we will explore the capabilities of MLflow, a comprehensive framework for the complete machine learning lifecycle. MLflow provides tools for tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\n",
    "\n",
    "In this demo, **we will focus on tracking and logging components of MLflow**. First, we will demonstrate how to track an experiment with MLflow and show various custom logging features including logging parameters, metrics, figures and arbitrary artifacts.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to*;\n",
    "\n",
    "* Manually log parameters, metrics, models, and figures with MLflow tracking.\n",
    "\n",
    "* Review an experiment using the MLflow UI.\n",
    "\n",
    "* Train a model using a Feature Store table as the modeling set.\n",
    "\n",
    "* Log training dataset with model in MLFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "664b6899-cdab-4b30-a632-3c0262c67446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da7d23f-4129-4c4f-9aa8-698472df1cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8e010e-217d-49e8-9eaf-021add301602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLFlow with Unity Catalog\n",
    "\n",
    "Databricks has support for MLflow with Unity Catalog (UC) integration and workspace based classic version. Although we won't go into the details of MLflow with UC in this demo, we will enable it. This means **models will be registered to UC**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "add4944e-54ca-468f-80ff-d2d0fffb601e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3193236c-f0bc-40ef-9678-6e056bc514aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "157d62f6-5944-400f-a2af-b5abe26ec92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5284484-0c76-449a-82c9-a9e8d08e9fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0554d161-11c4-4e05-97f6-3f5372e43894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ac0b522-eedd-44ef-9db6-439a14616e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "In this section, we will leverage the Feature Store to load the dataset for our machine learning experiment. Instead of directly reading from a CSV file, we will use the Feature Store setup to create a feature table and then read the data from it. This approach enhances reproducibility and ensures consistency in the datasets used for training and testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f697a02-0787-4e72-9806-e8a42883913b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "feature_dataset = mlflow.data.load_delta(\n",
    "    table_name = f\"{DA.catalog_name}.{DA.schema_name}.diabetes_binary\", \n",
    "    name = \"diabetes_binary\"\n",
    ")   \n",
    "feature_data_pd = feature_dataset.df.toPandas()\n",
    "# Drop the 'unique_id' column\n",
    "feature_data_pd = feature_data_pd.drop(\"unique_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d2d61d-ff76-4529-bf17-2b678ea8ba92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(feature_data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43d35fd5-281e-4f02-a831-4beb8a9f4fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert all columns in the DataFrame to the 'double' data type\n",
    "for column in feature_data_pd.columns:\n",
    "    feature_data_pd[column] = feature_data_pd[column].astype(\"double\")\n",
    "\n",
    "# If you want to see the updated types\n",
    "print(feature_data_pd.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9fd038-8d4d-4bf8-b680-77083223e02f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train / Test Split\n",
    "\n",
    "Before proceeding with model training, it's essential to split the dataset into training and testing sets. This step ensures that the model is trained on one subset of the data and evaluated on an independent subset, providing a reliable estimate of its performance on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c4b0ed-54ee-4b76-a9cd-020ab2e870d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"We have {feature_data_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into it's own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "X_all = feature_data_pd.drop(labels=target_col, axis=1)\n",
    "y_all = feature_data_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ad4d3fb-4cb8-45d9-bad1-d99ae520ed7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Fit and Log the Model\n",
    "\n",
    "Now that we have our training and testing sets, let's fit a Decision Tree model to the training data. During this process, we will use MLflow to log various aspects of the model, including parameters, metrics, and the resulting model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a0d6ba-5df6-4d16-9fc2-9374e32df340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtc_params = {\n",
    "  'criterion': 'gini',\n",
    "  'max_depth': 50,\n",
    "  'min_samples_split': 20,\n",
    "  'min_samples_leaf': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f181b90-0ae3-42e8-9ec2-20ca45af7a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this code, we use MLflow to start a run and log parameters such as the criterion and max_depth of the Decision Tree model. After fitting the model on the training data, we evaluate its performance on the test set and log the accuracy as a metric.\n",
    "\n",
    "**\uD83D\uDEA8 Important:** MLflow autologging is **enabled by default on Databricks**. This means you don't need to do anything for supported libraries. In the next section, we are disabling it and manually log params, metrics etc. just demonstrate how to do it manually when you need to log any custom model info. For example, we use [`log_info`](https://mlflow.org/docs/2.20.3/python_api/mlflow.html?highlight=log_input#mlflow.log_input ) and [`log_metric`](https://mlflow.org/docs/2.20.3/python_api/mlflow.html?highlight=log_input#mlflow.log_metric), which is an API used for logging dataset metadata in MLflow and metrics of interest (e.g. F1-score), respectively. \n",
    "\n",
    "> \uD83D\uDCA1 ****Note: We won't define the `experiment name`, all *runs* generated in this notebook will be logged under the notebook title.****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee386cef-f0c1-424d-a342-139087b5c637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# register models in UC\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30cb3b14-26d1-40b7-a20a-3ca709cdea02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import mlflow\n",
    "import mlflow.data\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# set the path for mlflow experiment\n",
    "mlflow.set_experiment(f\"/Users/{DA.username}/Demo-1.2-Model-Tracking-with-MLflow\")\n",
    "\n",
    "# turn off autologging\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.diabetes-predictions\"\n",
    "\n",
    "# start an MLFlow run\n",
    "with mlflow.start_run(run_name=\"Model Tracking Demo\") as run:\n",
    "  # log the dataset\n",
    "  mlflow.log_input(feature_dataset, context=\"source\")\n",
    "  mlflow.log_input(mlflow.data.from_pandas(X_train, source=feature_dataset.source), context=\"training\")\n",
    "  mlflow.log_input(mlflow.data.from_pandas(X_test, source=feature_dataset.source), context=\"test\")\n",
    "\n",
    "  # log our parameters\n",
    "  mlflow.log_params(dtc_params)\n",
    "\n",
    "  # fit our model\n",
    "  dtc = DecisionTreeClassifier(**dtc_params)\n",
    "  dtc_mdl = dtc.fit(X_train, y_train)\n",
    "\n",
    "  # define model signature\n",
    "  signature = infer_signature(X_all, y_all)\n",
    "\n",
    "  # log the model\n",
    "  mlflow.sklearn.log_model(\n",
    "    sk_model = dtc_mdl, \n",
    "    artifact_path=\"model-artifacts\",\n",
    "    signature=signature,\n",
    "    registered_model_name=model_name)\n",
    "  \n",
    "  # evaluate on the training set\n",
    "  y_pred = dtc_mdl.predict(X_train)\n",
    "  mlflow.log_metric(\"train_accuracy\", accuracy_score(y_train, y_pred))\n",
    "  mlflow.log_metric(\"train_precision\", precision_score(y_train, y_pred))\n",
    "  mlflow.log_metric(\"train_recall\", recall_score(y_train, y_pred))\n",
    "  mlflow.log_metric(\"train_f1\", f1_score(y_train, y_pred))\n",
    "\n",
    "  # evaluate on the test set\n",
    "  y_pred = dtc_mdl.predict(X_test)\n",
    "  mlflow.log_metric(\"test_accuracy\", accuracy_score(y_test, y_pred))\n",
    "  mlflow.log_metric(\"test_precision\", precision_score(y_test, y_pred))\n",
    "  mlflow.log_metric(\"test_recall\", recall_score(y_test, y_pred))\n",
    "  mlflow.log_metric(\"test_f1\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd041fea-42dc-4c17-8f4d-2ddce8b23d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At this point we can access all model details using the **`run.info`** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "195fb85e-fd86-452f-ad7a-a2f9f1dedb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f8425ae-4956-461d-bd12-0f8f6d0a0e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log Model Artifacts\n",
    "\n",
    "**In addition to logging parameters, metrics, and the model itself, we can also log artifacts—any files or data relevant to the run.** Let's set up an MLflow client to log artifacts after the run is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca3349c-f787-492c-8b68-7314620bda66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.client import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe85b5a7-3dcf-462a-990b-e2a56a37eae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Log Confusion Matrix\n",
    "\n",
    "The confusion matrix is a useful tool to visualize the classification performance of the model. It provides insights into the true positive, true negative, false positive, and false negative predictions. \n",
    "\n",
    "Let's create the confusion matrix and [**log it with MLflow** using **`log_figure`** function](https://mlflow.org/docs/2.20.3/traditional-ml/hyperparameter-tuning-with-child-runs/part2-logging-plots.html?highlight=log_figure ). This function logs in-memory figure objects directly to the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d11106a0-6388-4761-a298-dcf826a1ba3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Computing the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "# Creating a figure object and axes for the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plotting the confusion matrix using the created axes\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 0])\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Setting the title of the plot\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Now 'fig' can be used with MLFlow's log_figure function\n",
    "client.log_figure(run.info.run_id, figure=fig, artifact_file=\"confusion_matrix.png\")\n",
    "\n",
    "# Showing the plot here for demonstration\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7101938e-972a-441d-97b4-b7593a7b5b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Log Feature Importance\n",
    "\n",
    "Now, **let's examine and log the resulting model**. We'll extract and plot the feature importances inferred from the Decision Tree model to understand which data features are most critical for successful prediction.\n",
    "\n",
    "Similar to the previous figure, we will use **`log_figure`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86f61be9-3f9c-49d2-a3b4-73bb3c224d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Retrieving feature importances\n",
    "feature_importances = dtc_mdl.feature_importances_\n",
    "feature_names = X_train.columns.to_list()\n",
    "\n",
    "# Plotting the feature importances\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "y_pos = np.arange(len(feature_names))\n",
    "ax.bar(y_pos, feature_importances, align='center', alpha=0.7)\n",
    "ax.set_xticks(y_pos)\n",
    "ax.set_xticklabels(feature_names, rotation=45)\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_title('Feature Importances in Decision Tree Classifier')\n",
    "\n",
    "# log to mlflow\n",
    "client.log_figure(run.info.run_id, figure=fig, artifact_file=\"feature_importances.png\")\n",
    "\n",
    "# display here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a4e3f3-4f56-4de1-a26b-125b8d283ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Log Tree Structure\n",
    "\n",
    "Decision trees make splitting decisions on different features at different critical values, and visualizing the tree structure helps us understand the decision logic. We'll plot the branching tree structure for better interpretation.\n",
    "\n",
    "We can get the tree in text format or as a graph. **To log the text format we will use `log_artifact` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64be965b-2440-483f-8c2e-f4e65f7347fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The fitted DecisionTreeClassifier model has {dtc_mdl.tree_.node_count} nodes and is up to {dtc_mdl.tree_.max_depth} levels deep.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46563c83-a889-4641-8890-1c1e7ddf0134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is a very large decision tree, printing out the full tree logic, we can see it is vast and sprawling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a96041-4c43-4c43-b052-67d35fb2a633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "text_representation = export_text(dtc_mdl, feature_names=feature_names)\n",
    "print(text_representation)\n",
    "\n",
    "# save this to a local file\n",
    "tree_struct_filename = \"tree_structure.txt\"\n",
    "with open(tree_struct_filename,'w') as f:\n",
    "  f.write(text_representation)\n",
    "\n",
    "# log it to mlflow\n",
    "client.log_artifact(run.info.run_id, tree_struct_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b792b78e-8b85-4fcc-9b46-dea669679e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's create a visually better looking version of this tree and log it with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "743e3859-62dc-48ad-a5ab-6d73373c3835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# plot the tree structure\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "plot_tree(dtc_mdl, \n",
    "          feature_names=feature_names,\n",
    "          max_depth=2,\n",
    "          class_names=['0', '1'], \n",
    "          filled=True,\n",
    "          ax=ax)\n",
    "ax.set_title('Decision Tree Structure')\n",
    "\n",
    "# log it to mlflow\n",
    "client.log_figure(run.info.run_id, fig, \"decision_tree_structure.png\")\n",
    "\n",
    "# display it here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb75dfc9-23f8-408d-b11e-d3393bd997fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Review the Model via the UI\n",
    "\n",
    "\n",
    "To review the model and its details, follow these step-by-step instructions:\n",
    "\n",
    "+ **Step 1: Go to the \"Experiments\" Section:**\n",
    "\n",
    "  **Option 1:**\n",
    "  - Click the Experiment icon<img src= \"../Includes/images/experiment.png\" width=30>in the notebook’s right sidebar\n",
    "\n",
    "  - In the Experiment Runs sidebar, click the<img src= \"../Includes/images/external-link.png\" width=30>icon next to the date of the run. The MLflow Run page displays, showing details of the run, including parameters, metrics, tags, and a list of artifacts.\n",
    "\n",
    "    ![quick-start-nb-experiment](../Includes/images/quick-start-nb-experiment1.png)\n",
    "\n",
    "  **Option 2:**\n",
    "  - Click on **Experiments** in the left sidebar.\n",
    "  - Find the experiment name you specified in your MLflow run.\n",
    "  - Click on the experiment name to view the runs within that experiment.\n",
    "  - Locate the specific run you want to review.\n",
    "\n",
    "+ **Step 2: Reviewing Artifacts and Metrics:**\n",
    "\n",
    "  - Click on the run to see detailed information.\n",
    "  - Navigate to the \"Artifacts\" tab to view logged artifacts.\n",
    "  - Navigate to the \"Model metrics\" tab to view logged metrics.\n",
    "\n",
    "+ **Step 3: Viewing Confusion Matrix Image:**\n",
    "\n",
    "  - If you logged the confusion matrix as an artifact, you can find it in the \"Artifacts\" tab.\n",
    "  - You may find a file named \"confusion_matrix.png\" (or the specified artifact file name).\n",
    "  - Download or view the confusion matrix image.\n",
    "\n",
    "+ **Step 4: View models in the UI:**\n",
    "  - You can find details about the logged model under the<img src= \"../Includes/images/models-icon.png\" width=30>**Models** tab.\n",
    "  - Look for the model name you specified in your MLflow run (e.g., \"decision_tree_model\").\n",
    "\n",
    "+ **Explore Additional Options:**\n",
    "\n",
    "  - You can explore other tabs and options in the MLflow UI to gather more insights, such as \"Parameters,\" \"Tags,\" and \"Source.\"\n",
    "These instructions will guide you through reviewing and exploring the tracked models using the MLflow UI, providing valuable insights into the experiment results and registered models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43488552-1002-4573-a802-81a68a0c4c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This demo guided us through the process of building, evaluating, and interpreting a Decision Tree model for classification tasks. We started by preparing and splitting the dataset, then proceeded to train the model using a Feature Store table. We manually logged key parameters, metrics, and artifacts using MLflow tracking, facilitating comprehensive experiment tracking and reproducibility. We examined and logged the model's performance through a confusion matrix, analyzed feature importances, and visualized the resulting tree structure. By leveraging MLflow, we demonstrated effective model tracking and experimentation management, contributing to a more informed and accountable machine learning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f67e5d1-28a4-4278-887b-a9ca1be7d2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "1.2 Demo - Model Tracking with MLflow",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}