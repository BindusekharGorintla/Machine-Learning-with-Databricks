{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7636f227-f166-402b-9491-1956bf9f514d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05fdf667-e54a-4d9d-a825-8e674a19a081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Supervised Learning\n",
    "\n",
    "In this demo, we will guide you through using a supervised learning model. In particular, we will be working through how to train a regression model. In these demos, you will learn how to retrieve data and fit the model using Databricks Notebooks and track the model's development using MLflow. In addition, you will learn how to interpret results using visualization tools and various model metrics. \n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "* Fit a linear regression model on modeling data using the sklearn API.\n",
    "\n",
    "* Interpret the fit of an sklearn linear model’s coefficients and intercept.\n",
    "\n",
    "* Fit a decision tree model using sklearn API and training data.\n",
    "\n",
    "* Visualize an sklearn tree’s split points.\n",
    "\n",
    "* Identify which metrics are tracked by MLflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ff17578-e85d-4895-b318-76f66186ae47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b26096-bcc1-4d79-8e78-9cae619645cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e85f5418-d7f1-4728-9dd3-11ba9f0df4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c93649-c75f-4c3d-9b03-1e48633f6557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-1.1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a88b2b-c6b9-42e3-8921-fdd158b43c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41fcb2e-bf8f-4b80-a0c9-081a7b047de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "351b815b-04c9-4251-a186-f658d9e5e9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "In this section, we are going to prepare the dataset for our machine learning models. The dataset we'll be working with is the **California housing dataset**. \n",
    "\n",
    "The dataset has been loaded, cleaned and saved to a **feature table**. We will read data directly from this table.\n",
    "\n",
    "Then, we will split the dataset into **train and test** sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19705c3f-ba11-440a-83ee-edf8745b7214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "This dataset contains information about housing districts in California and **aims to predict the median house value** for California districts, based on various features.\n",
    "\n",
    "While data cleaning and feature engineering is out of the scope of this demo, we will only map the `ocean_proximity` field. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b02316-5fde-4b45-8ec1-07360b5a7ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# read data from the feature store\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.ca_housing\"\n",
    "feature_data_pd = fe.read_table(name=table_name).toPandas()\n",
    "feature_data_pd = feature_data_pd.drop(columns=['unique_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f2473d7-8ce2-44eb-9514-34bfa38444d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ocean_proximity_mapping = {\n",
    "    'NEAR BAY': 1,\n",
    "    '<1H OCEAN': 2,\n",
    "    'INLAND': 3,\n",
    "    'NEAR OCEAN': 4,\n",
    "    'ISLAND': 5  \n",
    "}\n",
    "\n",
    "# Replace values in the DataFrame\n",
    "feature_data_pd['ocean_proximity'] = feature_data_pd['ocean_proximity'].replace(ocean_proximity_mapping).astype(float)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "feature_data_pd = feature_data_pd.fillna(0)\n",
    "\n",
    "display(feature_data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f3fc602-7c87-4071-87e3-bb1d0e554325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train / Test Split\n",
    "\n",
    "Split the dataset into training and testing sets. This is essential for evaluating the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee4cb7a7-812a-4e90-b110-f9590636d410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"We have {feature_data_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into its own dataset\n",
    "target_col = \"median_house_value\"\n",
    "X_all = feature_data_pd.drop(labels=target_col, axis=1)\n",
    "y_all = feature_data_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.8, random_state=42)\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfa64065-925f-4911-83d3-ad12c841317b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Examine for Potential Co-linearity\n",
    "\n",
    "Now, let's examine the correlations between predictors to identify potential co-linearity. Understanding the relationships between different features can provide insights into the dataset and help us make informed decisions during the modeling process.\n",
    "\n",
    "Let's review the **correlation matrix** in **tabular format**. Also, we can create a **graph based on the correlation matrix** to easily inspect the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7e3df25-b96f-4e3b-97a9-91772c13c2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Combine X and y into a single DataFrame for simplicity\n",
    "data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = data.corr()\n",
    "\n",
    "# display correlation matrix\n",
    "pd.set_option('display.max_columns', 10)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db4b00e0-1f3f-4fc9-bba7-4ffad78f08c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display correlation matrix visually\n",
    "\n",
    "# Initialize figure\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(len(corr.columns)):\n",
    "        # Determine the color based on positive or negative correlation\n",
    "        color = 'blue' if corr.iloc[i, j] > 0 else 'red'\n",
    "\n",
    "        # don't fill in circles on the diagonal\n",
    "        fill = not( i == j )\n",
    "\n",
    "        # Plot the circle with size corresponding to the absolute value of correlation\n",
    "        plt.gca().add_patch(plt.Circle((j, i), \n",
    "                                       0.5 * np.abs(corr.iloc[i, j]), \n",
    "                                       color=color, \n",
    "                                       edgecolor=color,\n",
    "                                       fill=fill,\n",
    "                                       alpha=0.5))\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(-0.5, len(corr.columns) - 0.5)\n",
    "plt.ylim(-0.5, len(corr.columns) - 0.5)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xticks(np.arange(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(np.arange(len(corr.columns)), corr.columns)\n",
    "plt.title('Correlogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a910567a-5f82-49cc-9434-036832fd3514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fit a Regression Model\n",
    "\n",
    "To enhance the performance of our regression model, we'll scale our input variables so that they are on a common (standardized) scale. **Standardization ensures that each feature has a mean of 0 and a standard deviation of 1**, which can be beneficial for certain algorithms, including linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24eea73-1239-41d7-8a42-8000512e35c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# turn on autologging\n",
    "mlflow.sklearn.autolog(log_input_examples=True)\n",
    "\n",
    "# apply the Standard Scaler to all our input columns\n",
    "std_ct = ColumnTransformer(transformers=[(\"scaler\", StandardScaler(), [\"total_bedrooms\", \"total_rooms\", \"housing_median_age\", \"latitude\", \"longitude\", \"median_income\", \"population\", \"ocean_proximity\", \"households\"])])\n",
    "\n",
    "# pipeline to transform inputs and then pass results to the linear regression model\n",
    "lr_pl = Pipeline(steps=[\n",
    "  (\"tx_inputs\", std_ct),\n",
    "  (\"lr\", LinearRegression() )\n",
    "])\n",
    "\n",
    "# fit our model\n",
    "lr_mdl = lr_pl.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the test set\n",
    "predicted = lr_mdl.predict(X_test)\n",
    "test_r2 = r2_score(y_test, predicted)\n",
    "test_mse = mean_squared_error(y_test, predicted)\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_mape = mean_absolute_percentage_error(y_test, predicted)\n",
    "print(\"Test evaluation summary:\")\n",
    "print(f\"R^2: {test_r2}\")\n",
    "print(f\"MSE: {test_mse}\")\n",
    "print(f\"RMSE: {test_rmse}\")\n",
    "print(f\"MAPE: {test_mape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "956fdec3-9979-4d44-811b-ce82f4c5ef78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Examine Model Result\n",
    "\n",
    "Now, let's inspect the results of our linear regression model. We'll examine both the intercept and the coefficients of the fitted model. Additionally, we'll perform a **t-test on each coefficient to assess its significance in contributing to the overall model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3c682a4-2f3b-49e3-a1f0-645d47757ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc861b75-f43f-4614-80da-34afd85971b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Extracting coefficients and intercept\n",
    "coefficients = np.append([lr_mdl.named_steps['lr'].intercept_], lr_mdl.named_steps['lr'].coef_)\n",
    "coefficient_names = ['Intercept'] + X_train.columns.to_list()\n",
    "\n",
    "# Calculating standard errors and other statistics (this is a simplified example)\n",
    "# In a real scenario, you might need to calculate these values more rigorously\n",
    "n_rows, n_cols = X_train.shape\n",
    "X_with_intercept = np.append(np.ones((n_rows, 1)), X_train, axis=1)\n",
    "var_b = test_mse * np.linalg.inv(np.dot(X_with_intercept.T, X_with_intercept)).diagonal()\n",
    "standard_errors = np.sqrt(var_b)\n",
    "t_values = coefficients / standard_errors\n",
    "p_values = [2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1))) for i in t_values]\n",
    "\n",
    "# Creating a DataFrame for display\n",
    "summary_df = pd.DataFrame({'Coefficient': coefficients,\n",
    "                           'Standard Error': standard_errors,\n",
    "                           't-value': t_values,\n",
    "                           'p-value': p_values},\n",
    "                          index=coefficient_names)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa6172d6-c742-4430-b1c2-7f83a13e506a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_pos = np.arange(len(coefficient_names))\n",
    "plt.bar(y_pos, coefficients, align='center', alpha=0.7)\n",
    "plt.xticks(y_pos, coefficient_names, rotation=45)\n",
    "plt.ylabel('Coefficient Size')\n",
    "plt.title('Coefficients in Linear Regression')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3ca3049-8943-4fe2-80a2-bee2a01c42c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, this demonstration provided a comprehensive walkthrough of training a linear regression model using the scikit-learn library. We covered essential steps such as data preparation, model fitting, and result examination. Understanding the coefficients and intercept of the model is crucial for interpreting its predictive power. Moreover, we discussed the significance of each feature through t-tests, offering insights into the statistical relevance of predictors. Armed with this knowledge, practitioners can make informed decisions about the importance of variables in their regression models. This demo serves as a foundational guide for those seeking a practical understanding of linear regression modeling in a machine learning context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d70d88b0-71a1-4cf2-9045-d25cb81cd681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "1.1a Demo - Supervised Learning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}