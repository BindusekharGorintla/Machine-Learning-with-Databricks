{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcbb1d0d-81c8-4c7d-8dd5-2e9f4019e35c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20fe374-6919-4dcc-81d9-d54ccb56d6c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB - Build a  Feature Engineering Pipeline\n",
    "\n",
    "Welcome to the \"Build a Feature Engineering Pipeline\" lab! In this hands-on session, we'll dive into the essential steps of creating a robust feature engineering pipeline. From data loading and preparation to fitting a pipeline and saving it for future use, this lab equips you with fundamental skills in crafting efficient and reproducible machine learning workflows. Let's embark on the journey of transforming raw data into meaningful features for predictive modeling.\n",
    "\n",
    "**Lab Outline**\n",
    "\n",
    "+ **Task 1:** Load Dataset and Data Preparation\n",
    "  + **1.1.** Load Dataset\n",
    "  + **1.2.** Data Preparation\n",
    "+ **Task 2:** Split Dataset\n",
    "+ **Task 3:** Create Pipeline for Data Imputation and Transformation\n",
    "+ **Task 4:** Fit the Pipeline\n",
    "+ **Task 5:** Show Transformation Results\n",
    "+ **Task 6:** Save Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22df248e-d5e8-4b77-b5ec-12bbff15fc41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d71878-fe49-430b-99ed-ce9ec7c09e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e6ea71-535a-443a-94cd-5aef704040b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Lab Setup\n",
    "\n",
    "Before starting the Lab, run the provided classroom setup script. This script will establish necessary configuration variables tailored to each user. Execute the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2a340fb-1a01-489d-a2c2-17cfc468b84c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c610832-9e36-402f-a8f6-01b1b1559df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aed3efc-df00-4c59-9f8e-9f72a8e3fc5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf65f5db-ef4f-4901-8f3f-c1bf0f112d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Load Dataset and Data Preparation\n",
    "\n",
    "\n",
    "**1.1. Load Dataset:**\n",
    "+ Load a dataset with features that require imputation and transformation\n",
    "+ Display basic information about the dataset (e.g., schema, summary statistics)\n",
    "\n",
    "**1.2. Data Preparation:**\n",
    "\n",
    "+ Examine the dataset.\n",
    "+ Identify and discuss the features that need data preparation.\n",
    "+ Convert data types: Demonstrate converting data types for selected columns (e.g., String to Int, Int to Boolean).\n",
    "+ Remove a column: Discuss and remove a column with too many missing values.\n",
    "+ Remove outliers: Implement a threshold-based approach to remove outlier records for a specific column.\n",
    "+ Save cleaned dataset as \"silver table.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "504b542f-2b9f-4b1a-8010-146eb78cbcb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1.1. Load Dataset:**\n",
    "\n",
    "+ Load a dataset with features that require imputation and transformation\n",
    "+ Display basic information about the dataset (e.g., schema, summary statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d1b00d8-fe43-48f5-9e79-e318760775d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Set the path of the dataset\n",
    "dataset_path = f\"{DA.paths.datasets.cdc_diabetes}/cdc-diabetes/diabetes_binary_5050_raw.csv\"\n",
    "\n",
    "## Read the CSV file using the Spark read.csv function\n",
    "## Set the header parameter to True to indicate that the CSV file has a header\n",
    "## Set the inferSchema option to True for Spark to automatically detect the data types\n",
    "## Set the multiLine option to True to ensure that Spark reads multi-line fields properly\n",
    "cdc_df = <FILL_IN>\n",
    "\n",
    "## Display the resulting dataframe\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6593a56c-c9c0-4cca-8469-8a48c146f277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Set the path of the dataset\n",
    "dataset_path = f\"{DA.paths.datasets.cdc_diabetes}/cdc-diabetes/diabetes_binary_5050_raw.csv\"\n",
    "\n",
    "## Read the CSV file using the Spark read.csv function\n",
    "## Set the header parameter to True to indicate that the CSV file has a header\n",
    "## Set the inferSchema option to True for Spark to automatically detect the data types\n",
    "## Set the multiLine option to True to ensure that Spark reads multi-line fields properly\n",
    "cdc_df = spark.read.option(\"nullValue\", \"null\").csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "## Display the resulting dataframe\n",
    "display(cdc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cfb9a1b-5085-4a2c-ba78-a569554710ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1.2. Data Preparation:**\n",
    "\n",
    "+ Examine the dataset.\n",
    "+ Identify the features that need data preparation.\n",
    "+ Convert data types: Demonstrate converting data types for selected columns (e.g., String to Int, Double to Boolean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3885893-0011-4668-82eb-edc0c22fb464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Convert string columns to integer type\n",
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "## List of string columns to convert: (HighBP, CholCheck, PhysActivity)\n",
    "string_columns = <FILL_IN>\n",
    "\n",
    "## Iterate over string columns and cast to integer type\n",
    "for column in string_columns:\n",
    "   cdc_df = <FILL_IN>\n",
    "\n",
    "## Convert double columns to BooleanType (Diabetes_binary, hvyalcoholconsump)\n",
    "double_columns = <FILL_IN>\n",
    "for column in double_columns:\n",
    "    cdc_df = <FILL_IN>\n",
    "\n",
    "## Print the schema\n",
    "cdc_df.<FILL_IN>\n",
    "## Examine the printed schema to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0aad515-8371-4e4e-8eaa-53e0b20d565e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Convert string columns to integer type\n",
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "## List of string columns to convert\n",
    "string_columns = [\"HighBP\", \"CholCheck\", \"PhysActivity\"]\n",
    "\n",
    "## Iterate over string columns and cast to integer type\n",
    "for column in string_columns:\n",
    "    cdc_df = cdc_df.withColumn(column, col(column).cast(\"int\"))\n",
    "\n",
    "## Convert double columns to BooleanType\n",
    "double_columns = [\"Diabetes_binary\", \"hvyalcoholconsump\"]\n",
    "for column in double_columns:\n",
    "    cdc_df = cdc_df.withColumn(column, col(column).cast(BooleanType()))\n",
    "\n",
    "## Print the schema\n",
    "cdc_df.printSchema()\n",
    "## Examine the printed schema to verify the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36194050-f681-4729-b079-ebab1b63da50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "+ **Remove a column with too many missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d24b5de-11b2-4203-ac58-fc89fbe30213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, concat_ws, collect_list\n",
    "\n",
    "## First, get the count of missing values per column to create a singleton row DataFrame\n",
    "missing_cdc_df = cdc_df.agg(*[sum(when(col(c).isNull() | (length(trim(col(c).cast(\"string\"))) == 0) | lower(trim(col(c).cast(\"string\"))).isin([\"none\", \"null\"]), 1, ).otherwise(0) ).alias(c) for c in cdc_df.columns ] )\n",
    "\n",
    "## Define a helper function to transpose the DataFrame for better readability\n",
    "def TransposeDF(df, columns, pivotCol):\n",
    "    \"\"\"Helper function to transpose Spark DataFrame\"\"\"\n",
    "    columnsValue = <FILL_IN>\n",
    "    stackCols = ','.join(x for x in columnsValue)\n",
    "    df_1 = df.selectExpr(pivotCol, \"stack(\" + str(len(columns)) + \",\" + stackCols + \")\")\\\n",
    "              .select(pivotCol, \"col0\", \"col1\")\n",
    "    final_df = <FILL_IN>\n",
    "    return final_df\n",
    "\n",
    "## Transpose the missing_cdc_df for better readability\n",
    "missing_df_T = TransposeDF(spark.createDataFrame(<FILL_IN>)\n",
    "\n",
    "## Display the count of missing values per column\n",
    "display(missing_cdc_df)\n",
    "\n",
    "## Set a threshold for missing data to drop columns\n",
    "per_thresh = 0.6\n",
    "\n",
    "## Calculate the total count of rows in the DataFrame\n",
    "N = <FILL_IN>\n",
    "\n",
    "## Identify columns with more than the specified percentage of missing data\n",
    "to_drop_missing = <FILL_IN>\n",
    "\n",
    "## Drop columns with more than 60% missing data\n",
    "print(f\"Dropping columns {to_drop_missing} with more than {per_thresh * 100}% missing data\")\n",
    "cdc_no_missing_df = cdc_df.drop(*to_drop_missing)\n",
    "\n",
    "## Display the DataFrame after dropping columns with excessive missing data\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23559533-5995-49b4-958a-d5b938fe7d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import col, when, count, concat_ws, collect_list, length, trim, lower, sum\n",
    "\n",
    "## First, get the count of missing values per column to create a singleton row DataFrame\n",
    "missing_cdc_df = cdc_df.agg(*[sum(when(col(c).isNull() | (length(trim(col(c).cast(\"string\"))) == 0) | lower(trim(col(c).cast(\"string\"))).isin([\"none\", \"null\"]), 1, ).otherwise(0) ).alias(c) for c in cdc_df.columns ] )\n",
    "\n",
    "## Define a helper function to transpose the DataFrame for better readability\n",
    "def TransposeDF(df, columns, pivotCol):\n",
    "    \"\"\"Helper function to transpose Spark DataFrame\"\"\"\n",
    "    columnsValue = list(map(lambda x: str(\"'\") + str(x) + str(\"',\") + str(x), columns))\n",
    "    stackCols = ','.join(x for x in columnsValue)\n",
    "    df_1 = df.selectExpr(pivotCol, \"stack(\" + str(len(columns)) + \",\" + stackCols + \")\")\\\n",
    "              .select(pivotCol, \"col0\", \"col1\")\n",
    "    final_df = df_1.groupBy(col(\"col0\")).pivot(pivotCol).agg(concat_ws(\"\", collect_list(col(\"col1\"))))\\\n",
    "                   .withColumnRenamed(\"col0\", pivotCol)\n",
    "    return final_df\n",
    "\n",
    "## Transpose the missing_cdc_df for better readability\n",
    "missing_df_T = TransposeDF(spark.createDataFrame([{\"Column\": \"Number of Missing Values\"}]).join(missing_cdc_df), missing_cdc_df.columns, \"Column\")\n",
    "\n",
    "## Display the count of missing values per column\n",
    "display(missing_cdc_df)\n",
    "\n",
    "## Set a threshold for missing data to drop columns\n",
    "per_thresh = 0.6\n",
    "\n",
    "## Calculate the total count of rows in the DataFrame\n",
    "N = cdc_df.count()\n",
    "\n",
    "## Identify columns with more than the specified percentage of missing data\n",
    "to_drop_missing = [x.asDict()['Column'] for x in missing_df_T.select(\"Column\").where(col(\"Number of Missing Values\") / N >= per_thresh).collect()]\n",
    "\n",
    "## Drop columns with more than 60% missing data\n",
    "print(f\"Dropping columns {to_drop_missing} with more than {per_thresh * 100}% missing data\")\n",
    "cdc_no_missing_df = cdc_df.drop(*to_drop_missing)\n",
    "\n",
    "## Display the DataFrame after dropping columns with excessive missing data\n",
    "display(cdc_no_missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "946b7d9c-7d8f-417e-9dcf-6b6c94ae02c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "+ **Remove outliers: Implement a threshold-based approach to remove outlier records for a specific column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29cb417d-5b8a-4b71-85c4-3b21733a07ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define cutoff values\n",
    "MentHlth_cutoff = 0  ## Assuming MentHlth cannot be negative\n",
    "BMI_cutoff = 50  ## Reasonable upper limit for BMI\n",
    "\n",
    "## Apply both filters in a single step\n",
    "cdc_no_outliers_df = <FILL_IN>\n",
    ")\n",
    "\n",
    "## Display the count before and after removing outliers\n",
    "print(<FILL_IN>)\n",
    "\n",
    "## Display the DataFrame after removing outliers\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dac94df-160a-4ba0-942a-ace8f58de186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Define cutoff values\n",
    "MentHlth_cutoff = 0  # Assuming MentHlth cannot be negative\n",
    "BMI_cutoff = 50  # Reasonable upper limit for BMI\n",
    "\n",
    "## Apply both filters in a single step\n",
    "cdc_no_outliers_df = cdc_no_missing_df.filter(\n",
    "    (col(\"MentHlth\") >= MentHlth_cutoff) & (col(\"BMI\") <= BMI_cutoff)\n",
    ")\n",
    "\n",
    "## Display the count before and after removing outliers\n",
    "print(f\"Count - Before: {cdc_no_missing_df.count()} / After: {cdc_no_outliers_df.count()}\")\n",
    "\n",
    "## Display the DataFrame after removing outliers\n",
    "display(cdc_no_outliers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9273c63-d647-418c-b066-a52e8300b9db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "+ **Save the cleaned dataset as the \"silver table\" for further analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dd7d4b4-642f-48fb-92f7-16578db3a6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cdc_df_full = \"cdc_df_full\"\n",
    "\n",
    "## Save as DELTA table (silver)\n",
    "cdc_df_full_silver = <FILL_IN>\n",
    "cdc_no_outliers_df.write.mode(\"overwrite\").option(\"mergeSchema\", True).saveAsTable(cdc_df_full_silver)\n",
    "\n",
    "## Print the resulting DataFrame\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aea5203-0b83-4f80-baf8-3c57c89c9740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "cdc_df_full = \"cdc_df_full\"\n",
    "\n",
    "## Save as DELTA table (silver)\n",
    "cdc_df_full_silver = f\"{cdc_df_full}_silver\"\n",
    "cdc_no_outliers_df.write.mode(\"overwrite\").option(\"mergeSchema\", True).saveAsTable(cdc_df_full_silver)\n",
    "\n",
    "## Display the resulting DataFrame (optional)\n",
    "display(cdc_no_outliers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2cc3d71-0bec-4be3-b2d8-b9c1706c7dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Split Dataset\n",
    "\n",
    "**2.1. Split Dataset:**\n",
    "\n",
    "+ Split the cleaned dataset into training and testing sets in 80:20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a61d89-173a-4b0d-9767-1e1111afd716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = cdc_no_outliers_df.<FILL_IN>\n",
    "\n",
    "## Materialize the split DataFrames as DELTA tables\n",
    "train_df.write.mode(\"overwrite\").<FILL_IN>\n",
    "test_df.write.mode(\"overwrite\").<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6917515c-a47c-4b4d-a8b0-0cd076342b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = cdc_no_outliers_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "## Materialize the split DataFrames as DELTA tables\n",
    "train_df.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.cdc_df_train\")\n",
    "test_df.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.cdc_df_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756c83db-6445-4b80-93cb-8b6843da65ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, RobustScaler, VectorAssembler\n",
    "\n",
    "## Assuming 'train_data' is your training set DataFrame\n",
    "feature_columns = [\"income\"]  # Add your actual feature column names\n",
    "\n",
    "## Assemble features into a vector\n",
    "assembler = VectorAssembler(<FILL_IN>)\n",
    "train_assembled_df = assembler.transform(<FILL_IN>)\n",
    "test_assembled_df = assembler.transform(<FILL_IN>)\n",
    "\n",
    "## Define scaler and fit on the training set\n",
    "scaler = RobustScaler(inputCol=<FILL_IN>, outputCol=<FILL_IN>)\n",
    "scaler_fitted = scaler.<FILL_IN>\n",
    "\n",
    "## Apply to both training and test sets\n",
    "train_scaled_df = scaler_fitted.transform(<FILL_IN>)\n",
    "test_scaled_df = scaler_fitted.transform(<FILL_IN>)\n",
    "\n",
    "## Show the resulting DataFrames\n",
    "print(\"This is the Training set:\")\n",
    "<FILL_IN>\n",
    "print(\"This is the Testing set:\")\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6ac4b0-f471-45a6-94e6-ea6cec32e430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.ml.feature import StandardScaler, RobustScaler, VectorAssembler\n",
    "\n",
    "## Assuming 'train_data' is your training set DataFrame\n",
    "feature_columns = [\"income\"]  # Add your actual feature column names\n",
    "\n",
    "## Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"income_NUM_Col_assembled\")\n",
    "train_assembled_df = assembler.transform(train_df.select(*feature_columns))\n",
    "test_assembled_df = assembler.transform(test_df.select(*feature_columns))\n",
    "\n",
    "## Define scaler and fit on the training set\n",
    "scaler = RobustScaler(inputCol=\"income_NUM_Col_assembled\", outputCol=\"income_NUM_Col_scaled\")\n",
    "scaler_fitted = scaler.fit(train_assembled_df)\n",
    "\n",
    "## Apply to both training and test sets\n",
    "train_scaled_df = scaler_fitted.transform(train_assembled_df)\n",
    "test_scaled_df = scaler_fitted.transform(test_assembled_df)\n",
    "\n",
    "## Display the resulting DataFrames\n",
    "print(\"This is the Training set:\")\n",
    "train_scaled_df.show()\n",
    "print(\"This is the Testing set:\")\n",
    "test_scaled_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8af22d40-3368-43ed-b533-26386adf90d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Create Pipeline using Data Imputation and Transformation\n",
    "\n",
    "**3.1. Create Pipeline:**\n",
    "\n",
    "+ Create a pipeline with the following tasks:\n",
    "  + StringIndexer\n",
    "  + Imputer\n",
    "  + Scaler\n",
    "  + One-Hot Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "734a6b59-f35c-414c-b4a7-de6e826b3596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "## Get a list of integer & boolean columns\n",
    "integer_cols = [column.name for column in train_df.schema.fields if (column.dataType == IntegerType() or column.dataType == BooleanType())]\n",
    "\n",
    "## Loop through integer columns to cast each one to double\n",
    "## Convert integer columns to double\n",
    "for column in integer_cols:\n",
    "    train_df = train_df.withColumn(column, col(column).cast(<FILL_IN>))\n",
    "    test_df = test_df.withColumn(column, col(column).cast(<FILL_IN>))\n",
    "\n",
    "## Get a list of string, numeric columns\n",
    "string_cols = [c.name for c in train_df.schema.fields if c.dataType == <FILL_IN>]\n",
    "num_cols = [c.name for c in train_df.schema.fields if c.dataType == <FILL_IN>]\n",
    "\n",
    "## Get a list of columns with missing values\n",
    "## Numerical\n",
    "num_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in num_cols]\n",
    "row_dict_num = train_df.select(num_missing_values_logic).first().<FILL_IN>\n",
    "num_missing_cols = [column for column in row_dict_num if row_dict_num[column] > 0]\n",
    "\n",
    "## String\n",
    "string_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in string_cols]\n",
    "row_dict_string = train_df.select(string_missing_values_logic).first().<FILL_IN>\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "## Identify low and high cardinality columns\n",
    "cardinality_threshold = <FILL_IN>\n",
    "low_card_cols = []\n",
    "high_card_cols = []\n",
    "\n",
    "for col_name in string_cols:\n",
    "    unique_count = train_df.select(col_name).distinct().count()\n",
    "    if unique_count <= cardinality_threshold:\n",
    "        low_card_cols.append(col_name)\n",
    "    else:\n",
    "        high_card_cols.append(col_name)\n",
    "\n",
    "## Print columns with missing values and cardinality info\n",
    "print(f\"Numeric columns with missing values: {<FILL_IN>}\")\n",
    "print(f\"String columns with missing values: {<FILL_IN>}\")\n",
    "print(f\"Low-cardinality string columns (OHE allowed): {<FILL_IN>}\")\n",
    "print(f\"High-cardinality string columns (excluded from OHE): {<FILL_IN>}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6fe9844-6405-4f25-838e-f47d11ad5e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "## Get a list of integer & boolean columns\n",
    "integer_cols = [column.name for column in train_df.schema.fields if (column.dataType == IntegerType() or column.dataType == BooleanType())]\n",
    "\n",
    "## Loop through integer columns to cast each one to double\n",
    "for column in integer_cols:\n",
    "    train_df = train_df.withColumn(column, col(column).cast(\"double\"))\n",
    "    test_df = test_df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "## Get a list of string, numeric columns\n",
    "string_cols = [c.name for c in train_df.schema.fields if c.dataType == StringType()]\n",
    "num_cols = [c.name for c in train_df.schema.fields if c.dataType == DoubleType()]\n",
    "\n",
    "## Get a list of columns with missing values\n",
    "## Numerical\n",
    "num_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in num_cols]\n",
    "row_dict_num = train_df.select(num_missing_values_logic).first().asDict()\n",
    "num_missing_cols = [column for column in row_dict_num if row_dict_num[column] > 0]\n",
    "\n",
    "## String\n",
    "string_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in string_cols]\n",
    "row_dict_string = train_df.select(string_missing_values_logic).first().asDict()\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "## Identify low and high cardinality columns\n",
    "cardinality_threshold = 100\n",
    "low_card_cols = []\n",
    "high_card_cols = []\n",
    "\n",
    "for col_name in string_cols:\n",
    "    unique_count = train_df.select(col_name).distinct().count()\n",
    "    if unique_count <= cardinality_threshold:\n",
    "        low_card_cols.append(col_name)\n",
    "    else:\n",
    "        high_card_cols.append(col_name)\n",
    "\n",
    "## Print columns with missing values and cardinality info\n",
    "print(f\"Numeric columns with missing values: {num_missing_cols}\")\n",
    "print(f\"String columns with missing values: {string_missing_cols}\")\n",
    "print(f\"Low-cardinality string columns (OHE allowed): {low_card_cols}\")\n",
    "print(f\"High-cardinality string columns (excluded from OHE): {high_card_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ae2414d-6b71-48b5-ae59-d0e555b4abc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import required libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, RobustScaler, StringIndexer, OneHotEncoder\n",
    "\n",
    "## String/Cat Indexer\n",
    "## create an additional column to index low-cardinality string columns\n",
    "## these columns will retain their original null values via 'handleInvalid=\"keep\"'\n",
    "string_cols_indexed = [c + '_index' for c in low_card_cols]\n",
    "string_indexer = StringIndexer(inputCols=<FILL_IN>, outputCols=<FILL_IN>, handleInvalid=<FILL_IN>)\n",
    "\n",
    "## Imputer (same strategy for all double/indexes)\n",
    "## create a list of columns containing missing values\n",
    "## utilize the mode strategy to impute all the missing columns\n",
    "string_missing_cols_indexed = [c + '_index' for c in string_missing_cols if c in low_card_cols]\n",
    "to_impute = <FILL_IN>\n",
    "\n",
    "imputer = Imputer(inputCols=<FILL_IN>, outputCols=<FILL_IN>, strategy=<FILL_IN>)\n",
    "\n",
    "## Scale numerical\n",
    "## create a vector of numerical columns as an array in the 'numerical_assembled' column\n",
    "## robustly scale all the numerical_scaled values for this array in the 'numerical_scaled' column\n",
    "numerical_assembler = VectorAssembler(inputCols=<FILL_IN>, outputCol=<FILL_IN>)\n",
    "numerical_scaler = RobustScaler(inputCol=<FILL_IN>, outputCol=<FILL_IN>)\n",
    "\n",
    "## OHE categoricals\n",
    "## create an OHE encoder to turn the indexed low-cardinality string columns into binary vectors\n",
    "ohe_cols = [column + '_ohe' for column in low_card_cols]\n",
    "one_hot_encoder = OneHotEncoder(inputCols=<FILL_IN>, outputCols=<FILL_IN>, handleInvalid=<FILL_IN>)\n",
    "\n",
    "## Assembler (All)\n",
    "## re-collect all columns and create a 'features' column from them\n",
    "feature_cols = [\"numerical_scaled\"] + <FILL_IN>\n",
    "vector_assembler = VectorAssembler(inputCols=<FILL_IN>, outputCol=<FILL_IN>)\n",
    "\n",
    "## Instantiate the pipeline\n",
    "## instantiate a pipeline with all the above stages\n",
    "stages_list = [\n",
    "    string_indexer,\n",
    "    imputer,\n",
    "    numerical_assembler,\n",
    "    numerical_scaler,\n",
    "    one_hot_encoder,\n",
    "    vector_assembler\n",
    "]\n",
    "\n",
    "pipeline = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "895cafcb-7659-45df-ae51-233ed9dae950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## import required libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, RobustScaler, StringIndexer, OneHotEncoder\n",
    "\n",
    "## String/Cat Indexer\n",
    "## create an additional column to index low-cardinality string columns\n",
    "## these columns will retain their original null values via 'handleInvalid=\"keep\"'\n",
    "string_cols_indexed = [c + '_index' for c in low_card_cols]\n",
    "string_indexer = StringIndexer(inputCols=low_card_cols, outputCols=string_cols_indexed, handleInvalid=\"keep\")\n",
    "\n",
    "## Imputer (same strategy for all double/indexes)\n",
    "## create a list of columns containing missing values\n",
    "## utilize the mode strategy to impute all the missing columns\n",
    "string_missing_cols_indexed = [c + '_index' for c in string_missing_cols if c in low_card_cols]\n",
    "to_impute = num_missing_cols + string_missing_cols_indexed\n",
    "\n",
    "imputer = Imputer(inputCols=to_impute, outputCols=to_impute, strategy='mode')\n",
    "\n",
    "## Scale numerical\n",
    "## create a vector of numerical columns as an array in the 'numerical_assembled' column\n",
    "## robustly scale all the numerical_scaled values for this array in the 'numerical_scaled' column\n",
    "numerical_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"numerical_assembled\")\n",
    "numerical_scaler = RobustScaler(inputCol=\"numerical_assembled\", outputCol=\"numerical_scaled\")\n",
    "\n",
    "## OHE categoricals\n",
    "## create an OHE encoder to turn the indexed low-cardinality string columns into binary vectors\n",
    "ohe_cols = [column + '_ohe' for column in low_card_cols]\n",
    "one_hot_encoder = OneHotEncoder(inputCols=string_cols_indexed, outputCols=ohe_cols, handleInvalid=\"keep\")\n",
    "\n",
    "## Assembler (All)\n",
    "## re-collect all columns and create a 'features' column from them\n",
    "feature_cols = [\"numerical_scaled\"] + ohe_cols\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "## Instantiate the pipeline\n",
    "## instantiate a pipeline with all the above stages\n",
    "stages_list = [\n",
    "    string_indexer,\n",
    "    imputer,\n",
    "    numerical_assembler,\n",
    "    numerical_scaler,\n",
    "    one_hot_encoder,\n",
    "    vector_assembler\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=stages_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab43135-18c7-403c-8afc-9f578a7de014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 4: Fit the Pipeline\n",
    "**4.1. Fit the Pipeline:**\n",
    "\n",
    "+ Use the training dataset to fit the created pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bd4e765-c283-44f7-978e-ef4b4508bd2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Fit the pipeline using the training dataset\n",
    "pipeline_model = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "163a03b5-e62d-47d1-afb7-8d7a2d6c7e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Fit the pipeline using the training dataset\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d7cba0-50af-4f45-bf2e-7bef5c46407c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 5: Show Transformation Results\n",
    "**5.1. Transform Datasets:**\n",
    "\n",
    "+ Apply the fitted pipeline to transform the training and testing datasets.\n",
    "+ Apply these transformations to different sets (e.g., train, test, validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f65dfedf-be11-4ac4-90c3-e9dee46e9df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Transform both the training and test datasets using the previously fitted pipeline model\n",
    "train_transformed_df = pipeline_model.<FILL_IN>\n",
    "test_transformed_df = <FILL_IN>\n",
    "\n",
    "## Display the transformed features from the training dataset\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "338bc532-b1f3-4dc8-91b5-d1ce78ebfdeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Transform both the training and test datasets using the previously fitted pipeline model\n",
    "train_transformed_df = pipeline_model.transform(train_df)\n",
    "test_transformed_df = pipeline_model.transform(test_df)\n",
    "\n",
    "## Display the transformed features from the training dataset\n",
    "display(train_transformed_df.select(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af061976-8f83-4a73-8d21-9be6cfb97e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 6: Save Pipeline\n",
    "**6.1. Save Pipeline:**\n",
    "\n",
    "+ Save the fitted pipeline to the working directory.\n",
    "+ Explore the saved pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60639c0d-e7d9-4764-b818-1bf0928803c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Save the trained pipeline model to the specified directory in the working directory\n",
    "pipeline_model.<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cb35aaa-d429-4e68-bb8e-519de2d912da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Save the trained pipeline model to the specified directory in the working directory\n",
    "pipeline_model.write().overwrite().save(f\"{DA.paths.working_dir}/spark_pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39cd3eea-03af-437b-ac81-0fbf87de614f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Load the previously saved pipeline model from the specified directory in the working directory\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "## Load the pipeline model\n",
    "loaded_pipeline = <FILL_IN>\n",
    "\n",
    "## Display the stages of the loaded pipeline\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a34f57d-a3aa-4173-aba5-a04eead3d56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Load the previously saved pipeline model from the specified directory in the working directory\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "## Load the pipeline model\n",
    "loaded_pipeline = PipelineModel.load(f\"{DA.paths.working_dir}/spark_pipelines\")\n",
    "\n",
    "## Display the stages of the loaded pipeline\n",
    "loaded_pipeline.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d53bd19c-cb67-488b-b407-afcbbe9463cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, this lab demonstrated the crucial steps in preparing and transforming a dataset for machine learning. We covered data cleaning, splitting, and created a pipeline for tasks like imputation and scaling. Saving the pipeline ensures reproducibility, and these foundational concepts can be applied in various machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84089194-1d9f-4eaa-8a36-1b9af7d67514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.3 Lab - Build a  Feature Engineering Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}