{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "266dc82b-2f9d-4f27-aeff-83342f951e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a82aae41-4133-4bd9-ac97-8336ca9f5188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Custom Model Deployment with Model Serving\n",
    "\n",
    "Databricks Model Serving provides an easy way of deploying ML models for real-time inference. In some cases, you may need to deploy custom pipelines for your models. An example would be implementing a pre or post processing of the inference result. \n",
    "\n",
    "In this demo, we will demonstrate how you could use **MLflow's `PythonModel`** to implement a post-processing step for your model and serve it with Model Serving.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* Deploy a model with custom logic using Model Serving.\n",
    "\n",
    "* Create and manage serving endpoints using the API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62f79ba2-516e-45f4-ad69-c3df88a1a48b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7186e33d-e44e-4b04-adda-2258798409d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbba202-ebd3-4b88-93d1-77e91c7b19fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3cf8254-c306-4722-8dd0-bb0c2121f8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk --upgrade\n",
    "\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb5d2191-7849-4b3f-9c68-8f402853c934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "052666b0-4aff-4991-b952-46d253f15cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e152930c-6df0-4893-b138-c67a0cf0359b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e78afc5-91d8-411f-8bd9-ec16f61961dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this demonstration, we will use a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including internet subscription details such as subscription plans, monthly charges and payment methods.\n",
    "\n",
    "After loading the dataset, we will perform simple **data cleaning and feature selection**. \n",
    "\n",
    "In the final step, we will split the dataset into **features** and **response** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffdfd954-cde2-4f62-8359-1fa96e5afd20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import mlflow\n",
    "\n",
    "\n",
    "# Point to UC model registry\n",
    "client = mlflow.MlflowClient()\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Load dataset with spark\n",
    "shared_volume_name = 'telco' # From Marketplace\n",
    "csv_name = 'telco-customer-churn-missing' # CSV file name\n",
    "dataset_p_telco = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "\n",
    "# Dataset specs\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"] # Keeping numerical only for simplicity and demo purposes\n",
    "\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "# Convert all fields to double for spark compatibility\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", F.expr(\"try_cast(trim(TotalCharges) as double)\"))\\\n",
    "            .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast('double'))\\\n",
    "            .withColumn(\"Tenure\", col(\"tenure\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = telco_df.select(primary_key, *features)\n",
    "response_df = telco_df.select(primary_key, response)\n",
    "\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3fd554a-d662-47f7-b778-fdc4287fb5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Fit and Register a Custom Model\n",
    "\n",
    "Before we start model deployment process, we will **fit and register a custom model**. \n",
    "\n",
    "Deploying custom pipeline for models typically involves following steps;\n",
    "\n",
    "1. Declare **wrapper classes** for custom models\n",
    "\n",
    "2. Train base model\n",
    "\n",
    "3. Instantiate custom model using trained base model & log to registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f30cb78f-6a3c-428a-a141-c805aa2673b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Wrapper Class\n",
    "\n",
    "We will use MLflow's `PythonModel` class to create a custom pipeline. `predict` function of the class implements the custom logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "987dd90d-1fa4-44ac-ab61-9e7126944410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Model wrapper class to output labels and associated probabilities\n",
    "class CustomProbaModel(mlflow.pyfunc.PythonModel):\n",
    "    # Initialize model in the constructor\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    " \n",
    "    # Prediction function\n",
    "    def predict(self, context, model_input):\n",
    "        # Predict the probabilities and class\n",
    "        prediction_probabilities = self.model.predict_proba(model_input)\n",
    "        predictions = self.model.predict(model_input)\n",
    " \n",
    "        # Organize multiple outputs\n",
    "        class_labels = [\"No\", \"Yes\"]\n",
    "        result = pd.DataFrame(prediction_probabilities, columns=[f'prob_{label}' for label in class_labels])\n",
    "        result['prediction'] = predictions\n",
    "        \n",
    "        return result\n",
    "    \n",
    "# Dummy model outputting array\n",
    "class CustomCodeModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def predict(self, context, data):\n",
    "        return [ j for j in range(0, data.shape[0]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f4f19d7-f985-4e28-a7b2-83b315ed9f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train Base Model\n",
    "\n",
    "In this step, **we will fit the model as normal**.\n",
    "\n",
    "Next, and the most important step is **wrapping the model with custom class that we created**.Then, **wrapped model is logged with MLflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c4b4384-7f48-460c-92c2-4bcb81f00bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_pdf, Y_train_pdf, test_size=0.2, random_state=42)\n",
    " \n",
    "# Initialize and train DecisionTreeClassifier\n",
    "rf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ad1e193-35b9-498b-9fa7-5fdfb89264e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Wrap and Log the Custom Model\n",
    "Wrap the model and define the input and output schemas. From there, run and log the model using `pyfunc` flavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67f71a53-49fb-4829-9b1c-04b81be2f7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Wrap the model in the ModelWrapper\n",
    "wrapped_model = CustomProbaModel(rf)\n",
    "\n",
    "# Define the input and output schemas\n",
    "input_example = X_train[:1]\n",
    "output_example = wrapped_model.predict([],X_train[:1])\n",
    "signature = infer_signature(X_train[:1], output_example)\n",
    " \n",
    "# Start an MLflow run and log the model\n",
    "custom_model_name = f\"{DA.catalog_name}.{DA.schema_name}.custom_ml_model\"\n",
    "with mlflow.start_run(run_name=\"Custom Model Example\"):\n",
    "    mlflow.pyfunc.log_model(\"model\", \n",
    "                            python_model=wrapped_model, \n",
    "                            input_example=input_example, \n",
    "                            signature=signature,\n",
    "                            registered_model_name=custom_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dca2c85-56f9-4a54-90d8-e243c1390dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test Wrapped Model\n",
    "\n",
    "Before serving the model, let's test it and review the result to make sure the post-processing is implemented successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "950e4629-7abf-4da3-8cb2-a6b46cdd0788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the model from the run\n",
    "run_id = mlflow.last_active_run().info.run_id\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/model\")\n",
    " \n",
    "# Use the loaded model to predict on the test data\n",
    "y_test_ = loaded_model.predict(X_test)\n",
    "display(y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1a1b01a-d36f-4f95-b970-0bfc52dba14c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test custom code model\n",
    "custom_code_model = CustomCodeModel()\n",
    "y_cc_test = custom_code_model.predict([], X_train[:1])\n",
    "print(y_cc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "879fad9d-d310-492f-b332-7b119429c876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Serve the Custom Model\n",
    "\n",
    "Let's serve the model with Model Serving. Here, we will use the API to create the endpoint and serving the model.\n",
    "\n",
    "Please note that you could simply use the UI for this task too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c536fa-002f-422c-a04b-f65eae287afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import EndpointCoreConfigInput\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointTag\n",
    "\n",
    "\n",
    "# Create/Update endpoint and deploy model+version\n",
    "w = WorkspaceClient()\n",
    "endpoint_name = f\"ML_AS_03_Demo4_Custom_{DA.unique_name('_')}\"\n",
    "model_version = \"1\"\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": custom_model_name,\n",
    "            \"model_version\": model_version,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)\n",
    "\n",
    "try:\n",
    "  w.serving_endpoints.create_and_wait(\n",
    "    name=endpoint_name,\n",
    "    config=endpoint_config,\n",
    "    tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_custom_model_example\"})]\n",
    "  )\n",
    "  print(f\"Creating endpoint {endpoint_name} with models {custom_model_name} versions {model_version}\")\n",
    "\n",
    "except Exception as e:\n",
    "  if \"already exists\" in e.args[0]:\n",
    "    print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "\n",
    "  else:\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba8f752d-a818-446c-92a7-809e17dce023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query the Endpoint\n",
    "\n",
    "Now that the endpoint is ready, we can query it using the test-sample as shown below. Note that the `predictions` is returned as string (Yes/No) as we implemented in wrapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bce5595-c8cb-470d-b033-705d574047da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hard-code test-sample\n",
    "dataframe_records = [\n",
    "    {\"SeniorCitizen\": 0, \"tenure\":12, \"MonthlyCharges\":65, \"TotalCharges\":800},\n",
    "    {\"SeniorCitizen\": 1, \"tenure\":24, \"MonthlyCharges\":40, \"TotalCharges\":500}\n",
    "]\n",
    "\n",
    "print(\"Inference results:\")\n",
    "query_response = w.serving_endpoints.query(name=endpoint_name, dataframe_records=dataframe_records)\n",
    "print(query_response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "860eb5dc-4e47-4023-be3c-ecabcf99145d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we demonstrated how to build a custom model pipeline using MLflow's `PythonModel` class and serve it with Databricks Model Serving. Firstly, we defined the wrapper class with custom post-processing logic. Next, we fitted the model as usual and wrapped it with the custom model. Finally, we deployed the model with Model Serving and queried the serving endpoint using the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a44db72-32b5-48a9-80c9-8f60b9a9a1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "4.2 Demo - Custom Model Deployment with Model Serving",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}