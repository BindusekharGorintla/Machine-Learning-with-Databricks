{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1d89c67-a494-447f-ae79-fb650a4edf96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d830da-abf0-4162-8df8-55798d6a03fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Real-time Deployment with Model Serving (Offline Features)\n",
    "\n",
    "This demo focuses on **real-time model deployment** using Databricks Model Serving with **offline feature tables** stored in Unity Catalog (Delta tables with a primary key). You’ll train on offline features, deploy two model versions to one endpoint, and validate **A/B testing** traffic splits.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end you will be able to:\n",
    "- Use a **UC Delta feature table (offline)** for model training and evaluation.\n",
    "- Register models to Unity Catalog and manage **Champion/Challenger** aliases.\n",
    "- Create a **Model Serving** endpoint with **two versions** and configure a **traffic split** for A/B testing.\n",
    "- **Query** the endpoint (batching requests) and **visualize** prediction distribution by served model.\n",
    "- *(Optional)* Enable **inference table auto-capture** for request/response logging and observability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c370fb6f-cfaf-4f00-ae8c-1a95b57dbb90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f078eee-c0ba-4acd-85cf-2fc67545f913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**\n",
    "\n",
    "* Online Tables must be enabled for the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecffbd62-0230-4ca0-b533-53d1c7eadafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ddcd62-ff7b-4270-b9cc-52ac87ebb25d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25a620ec-fbdd-4f0a-9471-d2f612aecb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53b53055-039f-4f39-94c8-908557b16deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d100a11b-f890-42bd-9bea-8516e320a195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Offline Feature Tables for Real-Time Inferencing\n",
    "\n",
    "Before deploying a model for real-time inference, it’s important to understand the role of **feature tables** in Model Serving.\n",
    "\n",
    "A **feature table** is simply a **materialized Delta table in Unity Catalog** with a defined **primary key**. These tables store curated feature values used during both model training and inference.\n",
    "\n",
    "In this demo, we’ll focus entirely on **offline feature tables**, which:\n",
    "- Reside in Unity Catalog as Delta tables.  \n",
    "- Are typically refreshed on a schedule (for example, daily or hourly batch updates).  \n",
    "- Can be used directly by Model Serving endpoints without any additional configuration.  \n",
    "\n",
    "> In contrast to real-time or streaming “online” feature stores (now deprecated as standalone “online tables”), offline tables are suitable for most production workloads where features are precomputed and served from Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f652717-91af-48cf-bf5e-ea7769981eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Real-time Deployment With Offline Feature Tables\n",
    "Here we consider a scenario where you have already gone through the development process (data preparation, and model development) and you're ready to deploy a model with offline features. We will first look at deploying two models that were created as a part of the classroom setup - a champion model and a challenger model with aliases `champion` and `challenger`, respectively. \n",
    "\n",
    "We will serve our two models using a 50/50 traffic split for A/B Testing. First, Let's read in our data and explore its lineage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af94c75-8056-4939-a22f-705bf944948d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Inspect Offline The Feature Table and Model Versions\n",
    "\n",
    "For this demonstration, we will use a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including internet subscription details such as subscription plans, monthly charges and payment methods. \n",
    "\n",
    "As a part of the classroom setup for this course, a feature table was created called **features** that **did _not_ include feature lookups.** This is the table we are reading in during the next step.\n",
    "\n",
    "#### Lineage Inspection\n",
    "- Navigate to the catalog and schema used with this Vocareum environment (see the output from the previous cell).\n",
    "- Find the table called `features` and model called `ml_model`. \n",
    "  - Click on Lineage. \n",
    "  - Click on **See lineage graph** and inspect it. This will show the footprint of how the catalog assets were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f38c95e-d1e9-4b77-8af5-1ec083b27c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Read in Features and Response Variable from Feature Store\n",
    "\n",
    "Here we will read in our dataset and split between features and response variables. We will show how this can be performed with the Databricks SDK using the Feature Engineering Client.\n",
    "\n",
    "> #### What's the difference between `fe.read_table()` and `read.spark.table()`?\n",
    "Essentially, we use `fe.read_table()` whenever we are specifically working with feature tables stored within Feature Store and `spark.read.table()` for general-purpose reading. Note that `fe.read_table()` is part of the Databricks Feature Engineering API and integrates well with other Feature Store APIs like logging models (see ****Part 2: Real-Time Deployment with Online Feature Tables****). On the other hand, `spark.read.table()` is a broader Spark SQL method for reading data from any table within the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7651ce68-2576-4c8e-86b7-7c36cc1b6c81",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering Client Setup and Data Preparation"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# Initialize Feature Engineering Client\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Define primary key \n",
    "primary_key = \"customerID\"\n",
    "\n",
    "# Read in feature table\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.features\"\n",
    "X_train_df = fe.read_table(name=feature_table_name)\n",
    "X_train_pdf = X_train_df.drop(primary_key).toPandas()\n",
    "\n",
    "# Read in response table \n",
    "response_table_name = f\"{DA.catalog_name}.{DA.schema_name}.response\"\n",
    "Y_train_df = spark.read.table(response_table_name)\n",
    "Y_train_pdf = Y_train_df.drop(primary_key).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2d1f9e0-3191-4fce-93f4-6c05a78be980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Real-time A/B Testing with Model Serving\n",
    "\n",
    "Let's serve the two models we logged in the previous step using Model Serving. Model Serving supports endpoint management via the UI and the API. \n",
    "\n",
    "Below you will find instructions for using the UI and it is simpler method compared to the API. **In this demo, we will use the API to configure and create the endpoint**.\n",
    "\n",
    "**Both the UI and the API support querying created endpoints in real-time**. We will use the API to query the endpoint using a test-set.\n",
    "\n",
    "\n",
    "> #### What is A/B Testing? \n",
    "> A/B testing is a method to compare two versions of a model or system by splitting user traffic and measuring performance metrics to determine which version delivers better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be6c3a0-9757-4545-bee8-7b84af8f0e3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Unique Endpoint Name and Print It"
    }
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"ML_AS_03_Demo4_{DA.unique_name('_')}\"\n",
    "print(f\"Endpoint name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a871545-b065-44cb-8e89-f0635a455b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 1: Serve model(s) using UI\n",
    "\n",
    "After registering the (new version(s) of the) model to the model registry. To provision a serving endpoint via UI, follow the steps below.\n",
    "\n",
    "1. In the left sidebar, click **Serving**.\n",
    "\n",
    "2. To create a new serving endpoint, click **Create serving endpoint**.   \n",
    "  \n",
    "    a. In the **Name** field, enter the name printed above.  \n",
    "  \n",
    "    b. Click in the Entity field. A dialog appears. Go to **My models**, and then select the **'ml_model'** from the drop-down menus. \n",
    "\n",
    "    c. Click **Confirm**.\n",
    "  \n",
    "    d. In the **Version** drop-down menu, select the **version 1**.    \n",
    "  \n",
    "    e. Make sure the **Compute Scale-out** field is set to Custom.\n",
    "  \n",
    "    f. *[OPTIONAL]* to deploy another model (e.g. for A/B testing):\n",
    "    - Click on **+Add served entity**.\n",
    "    - Enter the above mentioned details as above, but use **version 2**.\n",
    "    - Set the traffic split to 50% for each model.\n",
    "  \n",
    "    g. Click **Create**. The endpoint page opens and the endpoint creation process starts.   \n",
    "  \n",
    "See the Databricks documentation for details ([AWS](https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html#ui-workflow)|[Azure](https://learn.microsoft.com/azure/databricks/machine-learning/model-serving/create-manage-serving-endpoints#--ui-workflow))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7adc5a24-6f6c-4ddf-afd2-e67462ad436c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 2: Serve Model(s) Using the Databricks Python SDK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d3de4c1-0252-4165-802e-9471fb56547e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Get Models to Serve\n",
    "\n",
    "In order to serve the model, we will initialize the MLflow client with `MLflowClient` and the workspace client with `WorkspaceClient`. We will configure the MLflow client to point to Unity Catalog instead of the Workspace with `set_registry_uri(\"databricks-uc\")`. The workspace client will be used to create the model serving endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "279fe3dd-fc04-43c3-9d63-8b1672420a89",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run to Initialize MLflow client and workspace client"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointTag\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "# Initialize MLflow client\n",
    "client = mlflow.MlflowClient()\n",
    "# Initialize workspace client\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba2ee11-c68d-49f5-b0c0-0b3b3aab754d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define variables that will be used for configuring the endpoint like `model_name`. The output from running the next cell will show version 1 of our model registered as the champion model and version 2 as being the challenger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36bf2e9-d37a-4d6d-b723-f6aeb531379c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve and Display Model Versions"
    }
   },
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = f\"dbacademy.{DA.schema_name}.ml_model\"\n",
    "# Parse model name from UC namespace\n",
    "served_model_name =  model_name.split('.')[-1]\n",
    "# Define the endpoint name\n",
    "endpoint_name = f\"ML_AS_03_Demo4_{DA.unique_name('_')}\"\n",
    "\n",
    "# Get version of our model registered to UC as a part of the classroom setup\n",
    "model_version_champion = client.get_model_version_by_alias(name=model_name, alias=\"Champion\").version # Get champion version\n",
    "model_version_challenger = client.get_model_version_by_alias(name=model_name, alias=\"Challenger\").version # Get challenger version\n",
    "\n",
    "\n",
    "print(f\"Model version Champion: {model_version_champion}\")\n",
    "print(f\"Model version Challenger: {model_version_challenger}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfa187d9-a3e1-42b0-96b5-09aaf62b9f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Configure\n",
    "\n",
    "Define our model serving endpoint with `endpoint_config`. The configuration below shows two versions of the same being deployed (`model_version_champion` and `model_version_challenger`) along with how to configure traffic during inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcefc191-bfd5-4b74-88fa-b6e1ddad7958",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Skip this cell if you created the endpoint using the UI."
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import EndpointCoreConfigInput\n",
    "\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version_champion,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version_challenger,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        },\n",
    "    ],\n",
    "    \"traffic_config\": {\n",
    "        \"routes\": [\n",
    "            {\"served_model_name\": f\"{served_model_name}-{model_version_champion}\", \"traffic_percentage\": 50},\n",
    "            {\"served_model_name\": f\"{served_model_name}-{model_version_challenger}\", \"traffic_percentage\": 50},\n",
    "        ]\n",
    "    },\n",
    "    \"auto_capture_config\":{\n",
    "        \"catalog_name\": DA.catalog_name,\n",
    "        \"schema_name\": DA.schema_name,\n",
    "        \"table_name_prefix\": \"db_academy\" # Name of the inference table\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92611e88-13cd-4be4-9a2b-990984b6ba90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Serve the endpoint\n",
    "Use the configuration just created to serve the model.\n",
    "> The time to create a model serving endpoint < 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16feca44-3974-4a6a-ae68-5cc348e8204a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Skip this cell if you created the endpoint using the UI."
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  w.serving_endpoints.create(\n",
    "    name=endpoint_name,\n",
    "    config=endpoint_config,\n",
    "    tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_fs_model_example\"})]\n",
    "  )\n",
    "  print(f\"Creating endpoint {endpoint_name} with models {model_name} versions {model_version_champion} & {model_version_challenger}\")\n",
    "\n",
    "except Exception as e:\n",
    "  if \"already exists\" in e.args[0]:\n",
    "    print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "\n",
    "  else:\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0001fec7-9147-4a1e-9e02-89690c879d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Verify Endpoint Creation\n",
    "\n",
    "Let's verify that the endpoint is created and ready to be used for inference using the `assert` command, which is used to check whether a given condition is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be952874-d172-4dbf-a7f4-1d9827f4eb75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run this to test if endpoint is ready"
    }
   },
   "outputs": [],
   "source": [
    "endpoint = w.serving_endpoints.wait_get_serving_endpoint_not_updating(endpoint_name)\n",
    "\n",
    "assert endpoint.state.config_update.value == \"NOT_UPDATING\" and endpoint.state.ready.value == \"READY\" , \"Endpoint not ready or failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fa3cdb0-5288-483b-b22d-eddd62f656c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query the Endpoint and Visualize\n",
    "\n",
    "Here we will use the training dataset to query our endpoint.\n",
    "\n",
    "1. Define the dataset to sample from.\n",
    "1. Query by batch to highlight model-split traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7268d4a3-4f92-47c4-a347-2a578785fd3c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sample 1,000 Records from X_train_pdf"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_records = X_train_pdf.iloc[:1000].to_dict(orient='records') #1k sample records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576e900b-92b3-4dd6-a4a7-1439991e8737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we will query in batches so we can see the traffic split per 100 rows (there are around 2000 rows in this dataset)\n",
    "\n",
    "To help visualize the A/B testing output, create a visual using the UI (you only need to do this once;  rerunning the cell will update the visualization). \n",
    "1. After running the next cell, select the + sign on the second table and select **Visualization**. \n",
    "1. The default visual should represent the Yes/No split per model.\n",
    "\n",
    "> Since the dataset we're working with is not very large, you might have to run the cell a few times to get a fairly close 50/50 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e341eedf-745b-4e5e-83c1-6ca7d1331c17",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Batch Processing and Aggregation of Model Predictions"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Inference results:\")\n",
    "\n",
    "batch_size = 100  # Number of records per batch\n",
    "num_batches = (len(dataframe_records) + batch_size - 1) // batch_size  # Total number of batches\n",
    "\n",
    "all_predictions = []\n",
    "all_models = []\n",
    "\n",
    "# Process data in batches\n",
    "for i in range(num_batches):\n",
    "    batch_records = dataframe_records[i * batch_size:(i + 1) * batch_size]  # Slice batch\n",
    "\n",
    "    # Query the model serving endpoint\n",
    "    query_response = w.serving_endpoints.query(name=endpoint_name, dataframe_records=batch_records)\n",
    "\n",
    "    # Collect predictions and model served details\n",
    "    all_predictions.extend(query_response.predictions)\n",
    "    all_models.extend([query_response.served_model_name] * len(query_response.predictions))  # Duplicate model name per prediction\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"prediction\": all_predictions,\n",
    "    \"model_served\": all_models\n",
    "})\n",
    "\n",
    "# Count occurrences of predictions\n",
    "count_results = results_df['prediction'].value_counts().reset_index()\n",
    "count_results.columns = ['prediction', 'count']\n",
    "\n",
    "# Display aggregated count of predictions\n",
    "display(count_results)\n",
    "\n",
    "# Aggregate count of predictions per model\n",
    "model_count_results = results_df.groupby([\"model_served\", \"prediction\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Display results grouped by model and prediction type\n",
    "display(model_count_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c600261d-7546-4c22-8e05-47a8875c7aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This demonstration showed how to deploy and serve machine learning models in real time using **Databricks Model Serving** with **offline feature tables** stored in Unity Catalog.  \n",
    "You learned how to:\n",
    "\n",
    "- Register and manage models with **Champion/Challenger** aliases.  \n",
    "- Configure a **Model Serving endpoint** with multiple model versions for **A/B testing**.  \n",
    "- Send inference requests and visualize traffic distribution between model versions.  \n",
    "\n",
    "This workflow highlights how Databricks simplifies real-time deployment and model experimentation using reliable, batch-refreshed **offline Delta feature tables**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2e9f432-5419-4a96-97ef-8dea04e4671b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5054076868876838,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4.1 Demo - Real-time Deployment with Model Serving",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}