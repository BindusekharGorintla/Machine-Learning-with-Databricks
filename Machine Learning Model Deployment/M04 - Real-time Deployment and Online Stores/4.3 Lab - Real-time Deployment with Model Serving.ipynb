{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca210f4-3edf-4a72-9c17-ebfce1d014e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b9aac75-3cbd-4314-bb58-1fc9ff869472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB - Real-time Deployment with Model Serving\n",
    "\n",
    "In this lab, you will deploy ML models with Databricks Model Serving using **offline feature tables** (Delta in Unity Catalog). This lab includes **two** sections.\n",
    "\n",
    "In the first section, you will deploy a model for real-time inference with Model Serving's **UI**. This section will demonstrate the most basic and simple way of deploying models with Model Serving.\n",
    "\n",
    "In the second section, you will deploy a model **programmatically using the Databricks SDK (API)**.\n",
    "\n",
    "For both sections, data preparation, model fitting and model registration are already done for you! You just need to focus on the deployment part.\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "* Simple real-time deployment\n",
    "  - **Task 1:** Serve the model using the UI\n",
    "  - **Task 2:** Query the endpoint\n",
    "\n",
    "* API-based real-time deployment \n",
    "  - **Task 3:** Create an offline feature table\n",
    "  - **Task 4:** Create a derived feature using a SQL function\n",
    "  - **Task 5:** Prepare the feature table for inference\n",
    "  - **Task 6:** (Optional) Define features with FeatureLookup/FeatureFunction for illustration\n",
    "  - **Task 7:** Create training set and fit the model (offline join)\n",
    "  - **Task 8:** Deploy the model\n",
    "  - **Task 9:** Query the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b05ba5b-9a71-43e6-a613-4d0bcc4f8b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2ce14b-9f2e-4e6e-b3bd-a5e9d264cc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d61a8138-cc14-4c92-b2a1-10f6f9ffbbdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the lab, run the provided classroom setup scripts. \n",
    "\n",
    "**\uD83D\uDCCC Note:** In this lab you will be using the Databricks SDK to create Model Serving endpoint. Therefore, you will need to run the next code block to **install `databricks-sdk`**. \n",
    "\n",
    "Before starting the lab, run the provided classroom setup script. This script will define configuration variables necessary for the lab. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b83ec16a-c176-4377-a794-2a667bd2e317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qq databricks-sdk databricks-feature-engineering==0.12.1\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b2a1451-7e66-47df-a63e-b2492e716703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a7edba3-f0ba-41f7-8393-f5b7449e0ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this lab, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5beb21cc-5dbe-4dd9-b52d-95d40f9a92cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5a5093f-409b-47ab-b948-42e4ca3d82b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data and Model Preparation\n",
    "\n",
    "Before you start the deployment process, you will need to fit and register a model. In this section, you will load dataset, fit a model and register it with UC.\n",
    "\n",
    "**Note:** All necessary code is provided, which means you don't need to complete anything in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d25a5282-fba7-4b1c-99a0-903c3b0839dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af332f8b-d803-4d29-a4ba-26a462c34d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "## Set the path of the dataset\n",
    "shared_volume_name = 'cdc-diabetes' # From Marketplace\n",
    "csv_name = 'diabetes_binary_5050split_BRFSS2015' # CSV file name\n",
    "dataset_path = f\"{DA.paths.datasets.cdc_diabetes}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "\n",
    "\n",
    "df = spark.read.csv(dataset_path, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "    .na.drop(how='any')\n",
    "\n",
    "df = df.withColumn(\"uniqueID\", monotonically_increasing_id())   # Add unique_id column\n",
    "\n",
    "## Dataset specs\n",
    "primary_key = \"uniqueID\"\n",
    "response = \"Diabetes_binary\"\n",
    "\n",
    "## Separate features and ground-truth\n",
    "features_df = df.drop(response)\n",
    "response_df = df.select(primary_key, response)\n",
    "\n",
    "## Convert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "610d168f-6876-426d-87b8-e114d228cac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Model Registry with UC\n",
    "\n",
    "Before we start model deployment, we need to fit and register a model. In this lab, **we will log models to Unity Catalog**, which means first we need to setup the **MLflow Model Registry URI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75bd3519-705d-48c6-b141-3120ebecd385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "## Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e5ef41-4517-4330-b29a-e2a17d0c938e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper Class for Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffcda25d-20ab-4978-ab7a-fd7c8f4fa4b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from mlflow.types.utils import _infer_schema\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_diabetes_model\" ## Use 3-level namespace\n",
    "\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get the latest model version as a string\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    model_version_list = [model_version_info.version for model_version_info in model_version_infos]\n",
    "    ## Convert to integers for correct numeric comparison\n",
    "    model_version_int_list = list(map(int, model_version_list))\n",
    "    ## Find the maximum and convert back to a string\n",
    "    return str(max(model_version_int_list))\n",
    "\n",
    "def fit_and_register_model(X, Y, model_name_=model_name, random_state_=42, model_alias=None, log_with_fs=False, training_set_spec_=None):\n",
    "    \"\"\"Helper function to train and register a decision tree model\"\"\"\n",
    "\n",
    "    clf = DecisionTreeClassifier(random_state=random_state_)\n",
    "    with mlflow.start_run(run_name=\"LAB4-Real-Time-Deployment\") as mlflow_run:\n",
    "\n",
    "        ## Enable automatic logging of input samples, metrics, parameters, and models\n",
    "        mlflow.sklearn.autolog(\n",
    "            log_input_examples=True,\n",
    "            log_models=False,\n",
    "            log_post_training_metrics=True,\n",
    "            silent=True)\n",
    "        \n",
    "        clf.fit(X, Y)\n",
    "\n",
    "        ## Log model and push to registry\n",
    "        if log_with_fs:\n",
    "            # Infer output schema\n",
    "            try:\n",
    "                output_schema = _infer_schema(Y)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "                output_schema = None\n",
    "            \n",
    "            ## Log using feature engineering client and push to registry\n",
    "            fe = FeatureEngineeringClient()\n",
    "            fe.log_model(\n",
    "                model = clf,\n",
    "                artifact_path = \"decision_tree\",\n",
    "                flavor = mlflow.sklearn,\n",
    "                training_set = training_set_spec_,\n",
    "                output_schema = output_schema,\n",
    "                registered_model_name = model_name_\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            signature = infer_signature(X, Y)\n",
    "            example = X[:3]\n",
    "            mlflow.sklearn.log_model(\n",
    "                clf,\n",
    "                artifact_path = \"decision_tree\",\n",
    "                signature = signature,\n",
    "                input_example = example,\n",
    "                registered_model_name = model_name_\n",
    "            )\n",
    "\n",
    "        ## Set model alias\n",
    "        if model_alias:\n",
    "            time.sleep(10) ## Wait 10secs for model version to be created\n",
    "            client.set_registered_model_alias(model_name_, model_alias, get_latest_model_version(model_name_))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0e8ee6e-3b2a-42d7-8c50-282b5e49e930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fit and Register the Model\n",
    "\n",
    "Before we start model deployment process, we will **fit and register a model**. The model's alias will be set to `Production` and it will be served with Databricks Model Serving in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ab03ff5-b8e6-4fa2-b41d-1ceec72de2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = fit_and_register_model(X_train_pdf, Y_train_pdf, model_name, 42, \"Production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af13812-82cd-445f-adac-257998f076b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simple Real-time Model Deployment\n",
    "\n",
    "Now that the model is registered and ready for deployment, the next step is to create a serving endpoint with Model Serving and serve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab9ca81-3fe5-4944-98cb-c7a22258652a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1: Serve the Model Using the UI\n",
    "\n",
    "Serve the **\"Production\"** model that we registered in the previous section using the following endpoint configuration.\n",
    "\n",
    "**Configuration:**\n",
    "\n",
    "* Name: `la4-1-diabetes-model`\n",
    "\n",
    "* Compute Size: `small` (CPU)\n",
    "\n",
    "* Autoscaling: `Scale to zero`\n",
    "\n",
    "* Tags: Define tags that might be meaningful for this deployment\n",
    "\n",
    "\n",
    "**\uD83D\uDCA1 Note:** Endpoint creation will take sometime. Therefore, you can work on the next section  while the endpoint is created for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab41ff9-1ceb-4479-8878-505dc9c59141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2: Query the Endpoint \n",
    "\n",
    "Test the model deployment using the **Query endpoint** feature in browsers. Use the provided **Example request** payload to use the model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401f7dc4-53f6-47cb-819d-ac016e447f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Real-time Model Deployment with Databricks Model Serving\n",
    "\n",
    "In this section, you will deploy a model using **Databricks Model Serving** with an **offline feature table** stored in Unity Catalog.  \n",
    "Unlike the previous section where you deployed through the **UI**, this time you will create and configure the serving endpoint programmatically using the **Databricks SDK (API)**.\n",
    "\n",
    "First, you will review the registered model that was trained and logged using features from a Delta table in Unity Catalog.  \n",
    "Then, you will deploy this model as a real-time serving endpoint using the API. Finally, you will query the endpoint to perform live inference using sample data records.\n",
    "\n",
    "This workflow demonstrates how to automate model deployment with Databricks Model Serving using **offline feature tables**, providing a foundation for scalable and reproducible production ML workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b2e9d89-0068-4bbb-a00b-4cee412b0dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3: Create an Offline Feature Table\n",
    "\n",
    "Let's create an **offline feature table** to store the features that will be used for model training and batch or real-time inference.  \n",
    "\n",
    "For this task, you will set up the feature table as follows:\n",
    "\n",
    "- The feature table will include **all feature columns** from the dataset  \n",
    "- Define a **primary key** for uniquely identifying each record  \n",
    "- Add a **description** for the table in Unity Catalog  \n",
    "\n",
    "This table will be stored as a **Delta table** in Unity Catalog and can later be accessed directly by Model Serving for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb67a133-2ea0-4c51-8fff-f50c284ac602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "## Define feature table name and initialize Feature Engineering client\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.diabetes_features\"\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "## Create the offline feature table\n",
    "fe.<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "142547fe-7d61-416f-aa07-de5d35186dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "## Define the feature table name in Unity Catalog\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.diabetes_features\"\n",
    "\n",
    "## Initialize Feature Engineering client\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "## Create the offline feature table\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    df=features_df,\n",
    "    primary_keys=[primary_key],\n",
    "    description=\"Offline feature table containing diabetes dataset features for model training and inference\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77b8bf35-a841-463e-836f-288ab5eeab35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 4: Create a Derived Feature Using SQL Function\n",
    "\n",
    "In this task, you will create a **derived feature** based on existing columns in the dataset.  \n",
    "Instead of directly using **Education** and **Income**, you will compute a new field called  \n",
    "**Education-Adjusted Income Index (EAI)** that represents a weighted interaction between the two.\n",
    "\n",
    "This field is calculated using the formula:  \n",
    "**`Education-Adjusted Income = Income Ã— Education`**\n",
    "\n",
    "*Note:* In real-world scenarios, correlated features such as income and education should be carefully examined for redundancy or multicollinearity. However, here the goal is to demonstrate how to define and register a simple SQL function in Unity Catalog that can be referenced during data processing or model training.\n",
    "\n",
    "The function should be structured as follows, using the variable names defined below:  \n",
    "- **Function name:** `eai_function`  \n",
    "- **Input:** `Income`, `Education`  \n",
    "- **Output:** `eai`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2588f6f-1b9d-4378-a413-88c143ca8f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create or replace a SQL function to compute the derived feature\n",
    "spark.sql(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b3679d1-c0d1-4b23-8c76-8eb58e1391de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Create or replace a SQL function to compute the derived feature\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION eai_function (Income DOUBLE, Education DOUBLE)\n",
    "RETURNS DOUBLE\n",
    "LANGUAGE PYTHON AS\n",
    "$$\n",
    "eai = Income * Education\n",
    "return eai\n",
    "$$\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbc5a1f5-cd61-45d2-8f83-c9148ffe75d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 5: Prepare the Feature Table for Inference\n",
    "\n",
    "In this task, you will make sure that the **offline feature table** you created earlier can be used directly for **model inference**.  \n",
    "This step ensures that the feature table is properly configured in Unity Catalog and that change tracking is enabled for incremental updates.\n",
    "\n",
    "**Perform the following steps:**\n",
    "\n",
    "* Enable **Change Data Feed (CDF)** on the feature table to allow incremental updates and lineage tracking  \n",
    "* Verify that the feature table is registered in Unity Catalog and available for use by Model Serving  \n",
    "\n",
    "The resulting table will remain an **offline Delta table**, suitable for both batch and real-time inference through Model Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce61dfd3-362d-4501-b1cb-8d2d48b55527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "## Initialize the Workspace client\n",
    "workspace = WorkspaceClient()\n",
    "\n",
    "## Enable Change Data Feed (CDF) on the offline feature table\n",
    "spark.sql(<FILL_IN>)\n",
    "\n",
    "## Retrieve and display table details from Unity Catalog\n",
    "feature_table_details = <FILL_IN>\n",
    "\n",
    "pprint(feature_table_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5982980e-f73d-4f53-a663-464dba5ed70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pprint import pprint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "## Initialize the Workspace client\n",
    "workspace = WorkspaceClient()\n",
    "\n",
    "## Enable Change Data Feed (CDF) on the offline feature table\n",
    "spark.sql(f\"\"\"ALTER TABLE {feature_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\"\")\n",
    "\n",
    "## Retrieve and display table details from Unity Catalog\n",
    "feature_table_details = workspace.tables.get(feature_table_name)\n",
    "\n",
    "pprint(feature_table_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc18512-c00a-4363-8d3b-70e6f474f061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 6: Define Features\n",
    "\n",
    "Now that you have an **offline feature table** and a registered SQL function, you will combine them so the model can use both the stored features and the derived feature during **training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36853795-de91-4959-93cf-50b75720462a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureLookup, FeatureFunction\n",
    "## Define combined feature lookup (offline table) and derived feature function\n",
    "features=[FILL_IN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21aa7aa4-e2f8-4434-8ed6-3809650151c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from databricks.feature_engineering import FeatureLookup, FeatureFunction\n",
    "\n",
    "## Define combined feature lookup (offline table) and derived feature function\n",
    "features = [\n",
    "    FeatureLookup(\n",
    "        table_name=feature_table_name,\n",
    "        lookup_key=primary_key\n",
    "    ),\n",
    "    FeatureFunction(\n",
    "        udf_name=\"eai_function\",\n",
    "        output_name=\"eai\",\n",
    "        input_bindings={\n",
    "            \"Education\": \"Education\",\n",
    "            \"Income\": \"Income\"\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dee4e93-9068-45f0-ae74-0dfbc29e77ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 7: Create Training Set and Fit the Model\n",
    "\n",
    "Now that all feature configuration is set and ready, create training set and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58e107fc-39b5-424d-be3f-b0f44f0bbfa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Build an offline training dataframe (join features + label; compute derived feature offline)\n",
    "training_df_offline = (\n",
    "    <FILL_IN>\n",
    ")\n",
    "\n",
    "## Convert to pandas\n",
    "X_train_pdf2 = training_df_offline.drop(primary_key, response).toPandas()\n",
    "Y_train_pdf2 = training_df_offline.select(response).toPandas()\n",
    "\n",
    "## Fit and register the model (OFFLINE: no FS metadata)\n",
    "model_name_2 = f\"{DA.catalog_name}.{DA.schema_name}.ml_diabetes_model_fe\"\n",
    "model_fe = fit_and_register_model(\n",
    "    X_train_pdf2,\n",
    "    Y_train_pdf2,\n",
    "    model_name_2,\n",
    "    20,\n",
    "    log_with_fs=False,          \n",
    "    training_set_spec_=None \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dc6ec5f-3dd8-4763-8ce9-f8a28ddd35db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Build an offline training dataframe (join features + label; compute derived feature offline)\n",
    "training_df_offline = (\n",
    "    features_df.join(response_df, on=primary_key, how=\"inner\")\n",
    "               .withColumn(\"eai\", F.col(\"Income\") * F.col(\"Education\"))\n",
    ")\n",
    "\n",
    "## Convert to pandas\n",
    "X_train_pdf2 = training_df_offline.drop(primary_key, response).toPandas()\n",
    "Y_train_pdf2 = training_df_offline.select(response).toPandas()\n",
    "\n",
    "## Fit and register the model (OFFLINE: no FS metadata)\n",
    "model_name_2 = f\"{DA.catalog_name}.{DA.schema_name}.ml_diabetes_model_fe\"\n",
    "model_fe = fit_and_register_model(\n",
    "    X_train_pdf2,\n",
    "    Y_train_pdf2,\n",
    "    model_name_2,\n",
    "    20,\n",
    "    log_with_fs=False,          \n",
    "    training_set_spec_=None \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e75d6f9-2d12-476d-ac87-b2821304027d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 8: Deploy the Model (Offline Features)\n",
    "\n",
    "Create a serving endpoint with the following configuration:\n",
    "\n",
    "* Autoscaling: `Scale-to-zero`\n",
    "* Compute size: `Small`\n",
    "\n",
    "**\uD83D\uDCA1 Note:** Endpoint creation will take some time. Please wait while the endpoint is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaa160cb-a0b8-4ccb-8cea-af23fd845bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, EndpointTag\n",
    "\n",
    "## Initialize Workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "## Get the model version that will be served (from the offline-logged model)\n",
    "model_version = <FILL_IN>\n",
    "\n",
    "## Define the endpoint configuration\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            <FILL_IN>\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "endpoint_config = <FILL_IN>\n",
    "\n",
    "## Define endpoint name\n",
    "endpoint_name = <FILL_IN>\n",
    "\n",
    "## Create the serving endpoint\n",
    "try:\n",
    "    w.<FILL_IN>(\n",
    "        name=<FILL_IN>,\n",
    "        config=<FILL_IN>,\n",
    "        tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"lab4_serve_offline_model\"})]\n",
    "    )\n",
    "    print(f\"Creating endpoint {endpoint_name} with model {model_name_2} version {model_version}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in e.args[0]:\n",
    "        print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ba125c-d5f5-4491-814f-bbbdcf21a709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, EndpointTag\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "## Get the model version that will be served (from the offline-logged model above)\n",
    "model_version = get_latest_model_version(model_name_2)\n",
    "\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": model_name_2,\n",
    "            \"model_version\": model_version,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)\n",
    "\n",
    "endpoint_name = f\"ML_AS_03_Lab4_{DA.unique_name('_')}\"\n",
    "\n",
    "try:\n",
    "    w.serving_endpoints.create_and_wait(\n",
    "        name=endpoint_name,\n",
    "        config=endpoint_config,\n",
    "        tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"lab4_serve_offline_model\"})]\n",
    "    )\n",
    "    print(f\"Creating endpoint {endpoint_name} with model {model_name_2} version {model_version}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in e.args[0]:\n",
    "        print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c9b39bc-972f-4bb7-afb3-a5098bf64b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 9: Query the Endpoint\n",
    "\n",
    "After the endpoint is created, it is time to test it. Use the following hard-coded test-sample to query the endpoint using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4926a9-1bc6-460a-bf80-56c21047ddf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample a few records for testing\n",
    "payload = X_train_pdf2.sample(3, random_state=42).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf9379d-9ae3-4e8a-a683-46a08af206b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Query the serving endpoint with test-sample\n",
    "query_response = w.serving_endpoints.<FILL_IN>\n",
    "\n",
    "print(\"Inference results:\", query_response.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1cd16c-d0b0-4da7-91c4-01dce0c089e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Query the serving endpoint\n",
    "query_response = w.serving_endpoints.query(\n",
    "    name=endpoint_name,\n",
    "    dataframe_records=payload\n",
    ")\n",
    "\n",
    "print(\"Inference results:\", query_response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17df92af-4dd2-41e7-b9fe-c6f18a89adb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Great job completing this lab!  \n",
    "In this lab, you successfully explored two key ways of deploying machine learning models with **Databricks Model Serving** using **offline feature tables**.\n",
    "\n",
    "- In the **first section**, you deployed a model using the **UI**, demonstrating the simplest way to expose a registered model for real-time inference.  \n",
    "- In the **second section**, you used the **Databricks SDK (API)** to automate model deployment. You created and prepared an offline feature table in Unity Catalog, defined a derived feature, trained and registered an offline model **without Feature Store metadata**, and deployed it to a real-time serving endpoint.  \n",
    "- Finally, you tested your endpoint by sending complete feature vectors for live inference.\n",
    "\n",
    "This workflow provides a foundation for building scalable, reproducible, and fully managed **real-time inference pipelines** using Databricks Model Serving with **offline Delta-based feature tables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d294295-efd0-463e-81fb-9066d4066100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "4.3 Lab - Real-time Deployment with Model Serving",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}