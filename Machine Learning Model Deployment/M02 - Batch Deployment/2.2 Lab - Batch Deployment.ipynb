{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fef54724-4286-4534-9e83-06451b01266d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86c17df7-d34d-4e47-8877-b204aba3432d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB - Batch Deployment\n",
    "\n",
    "Welcome to the \"Batch Deployment\" lab! This lab focuses on batch deployment of machine learning models using Databricks. You will engage in tasks related to model inference, model registry, and explore performance results for feature such as Liquid Clustering using `CLUSTER BY`.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "By the end of this lab, you will be able to;\n",
    "\n",
    "+ **Task 1: Load Dataset**\n",
    "    + Load Dataset\n",
    "    + Split the dataset into features and response sets\n",
    "\n",
    "+ **Task 2: Inference with feature table**\n",
    "\n",
    "    + Create Feature Table\n",
    "    + Setup Feature Lookups\n",
    "    + Fit and Register a Model with UC using Feature Table\n",
    "    + Perform batch inference using Feature Engineering's  **`score_batch`** method.\n",
    "\n",
    "+ **Task 3: Assess Liquid Clustering:**\n",
    "\n",
    "    + Evaluate the performance results for specific optimization techniques:\n",
    "        + Liquid Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fbef972-883c-491a-9b76-a59f70d46e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e98ca0c-8d15-4015-83d5-0bc899118cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e610bf5b-b99d-46d2-ae38-eae8d1f47162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the Lab, run the provided classroom setup script. This script will define configuration variables necessary for the lab. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3faa4c0-bc84-466e-8525-402879d0e72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7101f85-cb7a-4a5d-8765-32bf1b4e83d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this Lab, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e30a957-96d1-4fef-ad9c-3552f88d070c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da94c296-2f21-4fdb-bf92-9b8aff837528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Load Dataset\n",
    "\n",
    "+ Load a dataset:\n",
    "  + Define the dataset path\n",
    "  + Define the primary key (`customerID`), response variable (`Churn`), and feature variables (`SeniorCitizen`, `tenure`, `MonthlyCharges`, `TotalCharges`) for further processing.\n",
    "  + Read the dataset, casting relevant columns to the correct data types, and drop any rows with missing values\n",
    "+ Split the dataset into training and testing sets\n",
    "  + Separate the features and the response for the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5598ec44-160e-4407-84de-98b4cddb3b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "## Load dataset with spark\n",
    "shared_volume_name = 'telco' # From Marketplace\n",
    "csv_name = 'telco-customer-churn-missing' # CSV file name\n",
    "dataset_p_telco = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "\n",
    "## Features to use\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "## Read dataset (and drop nan)\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", F.expr(\"try_cast(trim(TotalCharges) as double)\"))\\\n",
    "            .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast('double'))\\\n",
    "            .withColumn(\"Tenure\", col(\"tenure\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "## Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = telco_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "## Separate features and ground-truth\n",
    "features_df = train_df.select<FILL_IN>\n",
    "response_df = train_df.select<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "749be1fd-3cdd-4a47-9d21-55a1245b2bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "## Load dataset with spark\n",
    "shared_volume_name = 'telco' # From Marketplace\n",
    "csv_name = 'telco-customer-churn-missing' # CSV file name\n",
    "dataset_p_telco = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "\n",
    "## features to use\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "## Read dataset (and drop nan)\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", F.expr(\"try_cast(trim(TotalCharges) as double)\"))\\\n",
    "            .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast('double'))\\\n",
    "            .withColumn(\"Tenure\", col(\"tenure\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "## Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = telco_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "## Separate features and ground-truth\n",
    "features_df = train_df.select(primary_key, *features)\n",
    "response_df = train_df.select(primary_key, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c56fd03-a7d4-4a8e-b788-79bb2b1d6f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 2: Inference with feature table\n",
    "In this task, you will perform batch inference using a feature table. Follow the steps below:\n",
    "\n",
    "+ **Step 1: Create Feature Table**\n",
    "\n",
    "+ **Step 2: Setup Feature Lookups**\n",
    "\n",
    "+ **Step 3: Fit and Register a Model with UC using Feature Table**\n",
    "\n",
    "+ **Step 4: Use the Model for Inference**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a4663f-5a2c-4f56-a99a-125e1f2d8768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1: Create Feature Table**\n",
    "  + Begin by creating a feature table that incorporates the relevant features for inference. This involves selecting the appropriate columns, performing any necessary transformations, and storing the resulting data in a feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b292135c-fe12-40f1-a18e-c2e976b9b4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "## Prepare feature set\n",
    "features_df_all = telco_df.select(primary_key, *features)\n",
    "\n",
    "## Feature table definition\n",
    "fe = FeatureEngineeringClient()\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.features\"\n",
    "\n",
    "## Drop table if exists\n",
    "try:\n",
    "    fe.drop_table(name=feature_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "## Create feature table\n",
    "fe.create_table(\n",
    "    <FILL_IN>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6326acc8-2357-4edd-908c-82129c0dd3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "## Prepare feature set\n",
    "features_df_all = telco_df.select(primary_key, *features)\n",
    "\n",
    "## Feature table definition\n",
    "fe = FeatureEngineeringClient()\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.features\"\n",
    "\n",
    "## Drop table if exists\n",
    "try:\n",
    "    fe.drop_table(name=feature_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "## Create feature table\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    df=features_df_all,\n",
    "    primary_keys=[primary_key],\n",
    "    description=\"Lab feature table\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710dd7c6-2231-4a22-82ac-5318bbc8e5d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2: Setup Feature Lookups**\n",
    "  + Set up a feature lookup to create a training set from the feature table. \n",
    "  + Specify the `lookup_key` based on the columns that uniquely identify records in your feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0136db-12bc-4de6-a3ba-f67f393eebfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureLookup\n",
    "\n",
    "fl_handle = <FILL_IN>\n",
    "\n",
    "## Create a training set based on feature lookup\n",
    "training_set_spec = fe.<FILL_IN>\n",
    "\n",
    "## Load training dataframe based on defined feature-lookup specification\n",
    "training_df = <FILL_IN>.load_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29cf268f-d3dd-442a-a7c2-f36bb1767f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from databricks.feature_engineering import FeatureLookup\n",
    "\n",
    "fl_handle = FeatureLookup(\n",
    "    table_name=feature_table_name,\n",
    "    lookup_key=[primary_key]\n",
    ")\n",
    "\n",
    "##  Create a training set based on feature lookup\n",
    "training_set_spec = fe.create_training_set(\n",
    "    df=response_df,\n",
    "    label=response,\n",
    "    feature_lookups=[fl_handle],\n",
    "    exclude_columns=[primary_key]\n",
    ")\n",
    "\n",
    "## Load training dataframe based on defined feature-lookup specification\n",
    "training_df = training_set_spec.load_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "656e9ec8-7204-4df7-975d-33ce742b5a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3: Fit and Register a Model with UC using Feature Table**\n",
    "  + Fit and register a Machine Learning Model using the created training set.\n",
    "  + Train a model on the training set and register it in the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fa9e36a-136f-401d-82d4-f34c4bcab5c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import warnings\n",
    "from mlflow.types.utils import _infer_schema\n",
    "\n",
    "## Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "## Helper function that we will use for getting latest version of a model\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])\n",
    "\n",
    "## Train a sklearn Decision Tree Classification model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Convert data to pandas dataframes\n",
    "X_train_pdf = training_df.drop(<FILL_IN>, <FILL_IN>).toPandas()\n",
    "Y_train_pdf = training_df.select(<FILL_IN>).toPandas()\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "## End the active MLflow run before starting a new one\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Batch-Deployment-lab-With-FS\") as mlflow_run:\n",
    "\n",
    "    Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True)\n",
    "    \n",
    "    clf.fit(<FILL_IN>, <FILL_IN>)\n",
    "\n",
    "    ## Infer output schema\n",
    "    try:\n",
    "        output_schema = _infer_schema(<FILL_IN>)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "        output_schema = None\n",
    "\n",
    "    model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_model\"\n",
    "    \n",
    "    ## Log using feature engineering client and push to registry\n",
    "    fe.<FILL_IN>\n",
    "\n",
    "    ## Set model alias (i.e. Champion)\n",
    "    client.set_registered_model_alias(<FILL_IN>, <FILL_IN>, <FILL_IN>(<FILL_IN>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12620811-0500-49c1-ae24-b4e7800a0706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import mlflow\n",
    "import warnings\n",
    "from mlflow.types.utils import _infer_schema\n",
    "\n",
    "## Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "## helper function that we will use for getting latest version of a model\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])\n",
    "\n",
    "## Train a sklearn Decision Tree Classification model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Convert data to pandas dataframes\n",
    "X_train_pdf = training_df.drop(primary_key, response).toPandas()\n",
    "Y_train_pdf = training_df.select(response).toPandas()\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "## End the active MLflow run before starting a new one\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Batch-Deployment-lab-With-FS\") as mlflow_run:\n",
    "\n",
    "    ## Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True)\n",
    "    \n",
    "    clf.fit(X_train_pdf, Y_train_pdf)\n",
    "\n",
    "    ## Infer output schema\n",
    "    try:\n",
    "        output_schema = _infer_schema(Y_train_pdf)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "        output_schema = None\n",
    "\n",
    "    model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_model\"\n",
    "    \n",
    "    ## Log using feature engineering client and push to registry\n",
    "    fe.log_model(\n",
    "        model=clf,\n",
    "        artifact_path=\"decision_tree\",\n",
    "        flavor=mlflow.sklearn,\n",
    "        training_set=training_set_spec,\n",
    "        output_schema=output_schema,\n",
    "        registered_model_name= model_name\n",
    "    )\n",
    "\n",
    "    ## Set model alias (i.e. Champion)\n",
    "    client.set_registered_model_alias(model_name, \"Champion\", get_latest_model_version(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4b7aaec-654f-40ee-ac0a-85d4533be35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 4: Use the Model for Inference**\n",
    "  + Utilize the feature engineering client's `score_batch()` method for inference.\n",
    "  + Provide the model URI and a dataframe containing primary key information for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a8da6a-6a60-4e68-824f-7a0eba6f4ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Load the model\n",
    "model_uri = f\"models:/<FILL_IN>\"\n",
    "\n",
    "## Define the test dataset\n",
    "test_features_df = test_df.select(\"FILL_IN\")\n",
    "\n",
    "## Perform batch inference using Feature Engineering's score_batch method\n",
    "result_df = fe.<FILL_IN>\n",
    "\n",
    "## Display the inference results\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8f02966-f20f-4ffa-8148-ad82d46b0c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Load the model\n",
    "model_uri = f\"models:/{model_name}@champion\"\n",
    "\n",
    "## Define the test dataset\n",
    "test_features_df = test_df.select(\"customerID\")\n",
    "\n",
    "## Perform batch inference using Feature Engineering's score_batch method\n",
    "result_df = fe.score_batch(\n",
    "    model_uri=model_uri,\n",
    "    df=test_features_df,\n",
    "    result_type='string'  # Update with the desired result type\n",
    ")\n",
    "\n",
    "## Display the inference results\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb790e2b-8835-45c2-a178-0d7619336863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Assess Liquid Clustering:\n",
    "\n",
    "Evaluate the performance results for specific optimization techniques, such as: Liquid Clustering Follow the step-wise instructions below:  \n",
    "+ **Step 1:** Create `batch_inference_liquid_clustering` table and import the following columns: `customerID`, `Churn`, `SeniorCitizen`, `tenure`, `MonthlyCharges`, `TotalCharges`, and `prediction`.\n",
    "+ **Step 2:**  Begin by assessing Liquid Clustering, an optimization technique for improving performance by physically organizing data based on a specified clustering column.\n",
    "+ **Step 3:**  Optimize the target table for Liquid Clustering.\n",
    "+ **Step 4:** Specify the `CLUSTER BY` clause with the desired columns (e.g., (customerID, tenure)) to enable Liquid Clustering on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce951260-9984-482c-9193-e232d6fddcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE batch_inference_liquid_clustering(\n",
    "  <FILL_IN> \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90411616-6880-4008-8b51-fa0f139cd6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sql\n",
    "CREATE OR REPLACE TABLE batch_inference_liquid_clustering(\n",
    "  customerID STRING,\n",
    "  Churn STRING,\n",
    "  SeniorCitizen DOUBLE,\n",
    "  tenure DOUBLE,\n",
    "  MonthlyCharges DOUBLE,\n",
    "  TotalCharges DOUBLE,\n",
    "  prediction STRING\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53ebe2e2-0084-4040-bc34-3246c8416933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE batch_inference_liquid_clustering;\n",
    "ALTER TABLE batch_inference_liquid_clustering\n",
    "CLUSTER BY (<FILL_IN>, <FILL_IN>);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e1b4e51-be73-4354-8cdb-6bbd602ed351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sql\n",
    "OPTIMIZE batch_inference_liquid_clustering;\n",
    "ALTER TABLE batch_inference_liquid_clustering\n",
    "CLUSTER BY (customerID, tenure);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fe4750-9c3a-43fa-9d20-5d4514d46704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This lab provides you with hands-on experience in batch deployment, covering model inference, Model Registry usage, and the impact of features like Liquid Clustering on performance. you will gain practical insights into deploying models at scale in a batch-oriented environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e333ac-ac34-4910-a658-5896aa5e365e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.2 Lab - Batch Deployment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}