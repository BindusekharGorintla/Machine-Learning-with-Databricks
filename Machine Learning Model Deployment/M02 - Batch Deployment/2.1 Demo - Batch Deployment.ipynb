{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4feef91-ffc7-4313-8c81-66f6a5cbd927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be467b1-fc89-4ff5-b996-3d250db0235c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Batch Deployment\n",
    "\n",
    "Batch inference is the most common way of deploying machine learning models.  This lesson introduces various strategies for deploying models using batch including Spark. In addition, we will show how to enable optimizations for Delta tables.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo you will be able to:*\n",
    "\n",
    "* Load a logged Model Registry model using `pyfunc`.\n",
    "\n",
    "* Compute predictions using `pyfunc` APIs.\n",
    "\n",
    "* Perform batch inference using Feature Engineering's `score_batch` method.\n",
    "\n",
    "* Materialize predictions into inference tables (Delta Lake).\n",
    "\n",
    "* Perform common write optimizations like liquid clustering, predictive optimization to maximize data skipping and on inference tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65c98c03-ac27-40e2-b65e-ac5c8832bcf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b39522ca-f59c-4248-a9ab-465d34f90119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**\n",
    "\n",
    "**\uD83D\uDEA8 Prerequisites:** \n",
    "* **Feature Engineering** and **Feature Store** are not the focus of this lesson. This course expect that you already know these topics. If not, you can check the **Data Preparation for Machine Learning** course.\n",
    "\n",
    "* Model development with MLFlow is not in the scope of this course. If you need to refresh your knowledge about model tracking and logging, you can check the **Machine Learning Model Development** course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd36d62-67f3-4a84-8093-7d0442d0c696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c37dd63-f820-4948-8556-28ddae573d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83a7c37f-1999-44b6-87b4-4344e84a6cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "538eb4cc-d999-443b-a100-521a90b09d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "135a46af-d9d7-4508-80ff-8d95673259eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this demonstration, we will utilize a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including gender, as well as internet subscription details such as subscription plans and payment methods.\n",
    "\n",
    "After loading the dataset, we will perform simple **data cleaning and feature selection**. \n",
    "\n",
    "In the final step, we will split the dataset into **features** and **response** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f007279-a32c-4a7e-8b38-83198832e27b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load dataset with spark\n",
    "shared_volume_name = 'telco' # From Marketplace\n",
    "csv_name = 'telco-customer-churn-missing' # CSV file name\n",
    "dataset_p_telco = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "\n",
    "# features to use\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"] # Keeping numerical only for simplicity and demo purposes\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", F.expr(\"try_cast(trim(TotalCharges) as double)\"))\\\n",
    "            .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast('double'))\\\n",
    "            .withColumn(\"Tenure\", col(\"tenure\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "# Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = telco_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = train_df.select(primary_key, *features)\n",
    "response_df = train_df.select(primary_key, response)\n",
    "\n",
    "# review the features dataset\n",
    "display(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57bc031a-fdc3-434c-9e36-3390ea1b5ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Batch Deployment - Without Feature Store\n",
    "\n",
    "This demo will cover two main batch deployment methods. The first method is deploying models without a feature table. For the second method, we will use a feature table to train the model and later use the feature table for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18f208b8-68e3-47ee-aa9a-8b3891db3943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Setup Model Registry with UC\n",
    "\n",
    "Before we start model deployment, we need to fit and register a model. In this demo, **we will log models to Unity Catalog**, which means first we need to setup the **MLflow Model Registry URI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00f15f0b-9236-4bd9-a587-539826c1f4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# helper function that we will use for getting latest version of a model\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78e4abf2-892a-4e71-8504-b88717e8c000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fit and Register a Model with UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fcd8ef8-da30-43c0-9255-1e613a2aedf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train a sklearn Decision Tree Classification model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# Use 3-level namespace for model name\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_model\" \n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Batch-Deployment-Demo\") as mlflow_run:\n",
    "\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True)\n",
    "    \n",
    "    clf.fit(X_train_pdf, Y_train_pdf)\n",
    "\n",
    "    # Log model and push to registry\n",
    "    signature = infer_signature(X_train_pdf, Y_train_pdf)\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf,\n",
    "        artifact_path=\"decision_tree\",\n",
    "        signature=signature,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Set model alias (i.e. Baseline)\n",
    "    client.set_registered_model_alias(model_name, \"Baseline\", get_latest_model_version(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4106eac-c25f-495d-9385-beb3d6a58b40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Use the Model for Inference \n",
    "\n",
    "Now that our model is ready in model registry, we can use it for inference. In this section we will use the model for inference directly on a spark dataframe, which is called **batch inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "385eaa36-2087-4597-a669-e9091230725e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load the Model\n",
    "\n",
    "Loading a model from UC-based model registry is done by getting a model using **alias** and **version**. \n",
    "\n",
    "After loading the model, we will create a **`spark_udf`** from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8100bb-b330-494a-bf7a-39131bff3c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_model_version = client.get_model_version_by_alias(name=model_name, alias=\"baseline\").version\n",
    "model_uri = f\"models:/{model_name}/{latest_model_version}\" # Should be version 1\n",
    "# model_uri = f\"models:/{model_name}@baseline # uri can also point to @alias\n",
    "predict_func = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17483b98-af28-4c94-9536-38ba28bae62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Inference\n",
    "\n",
    "Next, we will simply use the created function for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd79403b-2391-4b24-8ef2-bceee2a5d633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare test dataset\n",
    "test_features_df = test_df.select(primary_key, *features)\n",
    "\n",
    "# make prediction\n",
    "prediction_df = test_features_df.withColumn(\"prediction\", predict_func(*test_features_df.drop(primary_key).columns))\n",
    "\n",
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8600b7c-632c-444c-8929-3d85371d6159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Batch Deployment - With Feature Store \n",
    "\n",
    "In the previous section we trained and registered a model using Spark dataframe. In some cases, you will need to use features from a feature store for training and inference. \n",
    "\n",
    "In this section we will demonstrate how to train and deploy a model using Feature Store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f017f161-6829-4e41-a9dd-bf8dacac441c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Feature Table\n",
    "\n",
    "Let's create a feature table based on the `features_df` that we created before. Please note that we will be using **Feature Store with Unity Catalog**, which means we need to use **`FeatureEngineeringClient`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c21fd90-4bf4-4465-bddd-51c2f81c5959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# prepare feature set\n",
    "features_df_all = telco_df.select(primary_key, *features)\n",
    "\n",
    "# feature table definition\n",
    "fe = FeatureEngineeringClient()\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.features\"\n",
    "\n",
    "#drop table if exists\n",
    "try:\n",
    "    fe.drop_table(name=feature_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create feature table\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    df=features_df_all,\n",
    "    primary_keys=[primary_key],\n",
    "    description=\"Example feature table\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852f3762-197a-4d84-b269-a65e704e3ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Feature Lookups\n",
    "\n",
    "In order to create a training set from the feature table, we need to define a *feature lookup*. This will be used for creating training set from the feature table. \n",
    "\n",
    "Note that the **`lookup_key`** is used for matching records in feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b036b58-7c80-4eb5-b7f8-66011a50749c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create training set based on feature lookup\n",
    "from databricks.feature_engineering import FeatureLookup\n",
    "\n",
    "fl_handle = FeatureLookup(\n",
    "    table_name=feature_table_name,\n",
    "    lookup_key=[primary_key]\n",
    ")\n",
    "\n",
    "training_set_spec = fe.create_training_set(\n",
    "    df=response_df,\n",
    "    label=response,\n",
    "    feature_lookups=[fl_handle],\n",
    "    exclude_columns=[primary_key]\n",
    ")\n",
    "\n",
    "# Load training dataframe based on defined feature-lookup specification\n",
    "training_df = training_set_spec.load_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3b63e68-7c6a-409e-ae48-99ecde36a507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fit and Register a Model with UC using Feature Table\n",
    "\n",
    "After creating the training set, **model training and registering is the same as the previous step**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5222c116-84c3-4e7f-853f-0d8cee4ade31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from mlflow.types.utils import _infer_schema\n",
    "    \n",
    "    \n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf2 = training_df.drop(primary_key, response).toPandas()\n",
    "Y_train_pdf2 = training_df.select(response).toPandas()\n",
    "clf2 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Batch-Deployment-Demo-With-FS\") as mlflow_run:\n",
    "\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True)\n",
    "    \n",
    "    clf2.fit(X_train_pdf, Y_train_pdf)\n",
    "\n",
    "    # Infer output schema\n",
    "    try:\n",
    "      output_schema = _infer_schema(Y_train_pdf)\n",
    "    except Exception as e:\n",
    "      warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "      output_schema = None\n",
    "    \n",
    "    # Log using feature engineering client and push to registry\n",
    "    fe.log_model(\n",
    "        model=clf2,\n",
    "        artifact_path=\"decision_tree\",\n",
    "        flavor=mlflow.sklearn,\n",
    "        training_set=training_set_spec,\n",
    "        output_schema=output_schema,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Set model alias (i.e. Champion)\n",
    "    client.set_registered_model_alias(model_name, \"Champion\", get_latest_model_version(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "996c1fcc-d1fe-4195-9c14-83d0ab5de932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Use the Model for Inference\n",
    "\n",
    "Inference for models that are registered with a Feature Store table are different than inference with a Spark DataFrame. For inference, we will use **feature engineering client's `.score_batch()` method**. This method takes **a model URI** and **DataFrame with a primary key**.\n",
    "\n",
    "> **How does the function know which feature table to use?** If you visit **Artifacts** section of registered model, you will see a **`data`** folder is registered with the model. Also, model file includes **`data: data/feature_store`** statement to define feature data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d727feb8-edd3-412a-a38f-485ac34e70a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "champion_model_uri = f\"models:/{model_name}@champion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c21b75c8-7ad6-4d22-a91f-79486a24631d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare lookup dataset\n",
    "lookup_df = test_df.select(\"customerID\")\n",
    "\n",
    "# predict in batch using lookup df\n",
    "prediction_fe_df = fe.score_batch(\n",
    "    model_uri=champion_model_uri,\n",
    "    df=lookup_df,\n",
    "    result_type='string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f562e016-001c-45e8-b6a6-779483afce91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Join with the `test_df` DataFrame to compare the `prediction` and `churn` columns. Remember, we are _predicting_ churn in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e75e5f0c-8b4d-47b0-8808-c9ec4bbd67df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join predction_df_df and test_df on customerID and only keep columns from prediction_fe_df\n",
    "prediction_fe_df = prediction_fe_df.join(test_df.select([\"customerID\",\"Churn\"]), on = \"customerID\", how = \"left\")\n",
    "display(prediction_fe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9696bc40-9100-45ed-a695-9e4134499d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(prediction_fe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af56ec2-5b6c-412d-b214-f4e02e925985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance Considerations\n",
    "\n",
    "There are many possible (write) optimizations that Delta Lake can offer such as:\n",
    "- **Partitioning:** stores data associated with different categorical values in different directories.\n",
    "- **Z-Ordering (Legacy):** colocates related information in the same set of files.\n",
    "- **Liquid Clustering (Recommended):** replaces both above-mentioned  methods to simplify data layout decisions and optimize query performance.\n",
    "- **Predictive Optimizations:** removes the need to manually manage maintenance operations for Delta tables on Databricks.\n",
    "\n",
    "In this demo, we will show the last two options; liquid clustering and predictive optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5bc30c3-e7b8-415e-9751-4c86339fd710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {DA.catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {DA.schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b7b2487-8050-4d99-b342-f30e3a389db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Enable Predictive Optimization** at the schema level (can also be done at catalog level) is automatically enabled as a part of the Workspace setup and you do not have the proper permissions to enable it or disable it at the schema level as a user of this demo. However, the code to do so is provided here for completeness: \n",
    "\n",
    "```spark.sql(f\"ALTER SCHEMA {DA.catalog_name}.{DA.schema_name} ENABLE PREDICTIVE OPTIMIZATION;\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c395776e-4666-4795-a330-2b0ea77cec84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create inference table (where batch scoring jobs would be materialized) and enable liquid clustering on using `CLUSTER BY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4acbfcd-67df-43ca-bf6f-741aa860a138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE batch_inference(\n",
    "  customerID STRING\n",
    " ,Churn STRING\n",
    " ,SeniorCitizen DOUBLE\n",
    " ,tenure DOUBLE\n",
    " ,MonthlyCharges DOUBLE\n",
    " ,TotalCharges DOUBLE\n",
    " ,prediction STRING)\n",
    "CLUSTER BY (customerID, tenure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bb221ff-20b5-4c84-a38c-40b3b0494b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "  prediction_fe_df.write\n",
    "  .mode(\"append\")\n",
    "  .option(\"mergeSchema\", True)\n",
    "  .saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.batch_inference\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79afa1d1-c4ac-4a76-82df-e3321325412e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Manually optimize table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c6ecc7b-3003-47e1-a956-7e442eff0814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ANALYZE TABLE batch_inference COMPUTE STATISTICS FOR ALL COLUMNS;\n",
    "OPTIMIZE batch_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de680719-6267-4e4a-b03a-6853a1c8c4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Review the `batch_inference` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4281a512-e58b-45f2-aab9-9e2f5cd4e178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "* EXCEPT(Churn),\n",
    "Churn\n",
    "FROM batch_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c5683a-60c0-450e-b65b-49cad85c6977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this demo, we presented two main batch deployment methods using MLflow for model tracking and logging with Unity Catalog. In the first approach, we trained and registered a model without a feature table, reloading it for inference through a Spark UDF. The second method involved training a model with a feature table, registering it in the model registry, and using a look-up key for data retrieval during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "753a0d4b-c16a-4894-b5ad-6752c63abf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.1 Demo - Batch Deployment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}