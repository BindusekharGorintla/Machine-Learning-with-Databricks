{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7282025d-1a2c-4a3c-8e9b-b68ddac52b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.17.2 importlib-metadata==6.8.0 cloudpickle==2.0.0 zipp==3.16.2\n",
    "%pip install --ignore-installed Jinja2==3.1.2 markupsafe==2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66080dcc-557c-499c-bad0-0f6b3c5b4ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**\uD83D\uDEA8 Warning: Please don't run this notebook directly. This notebook must be used when creating the pipeline. Follow the instructions listed in the \"3.1.a - Pipeline Deployment\" notebook.**\n",
    "\n",
    "**\uD83D\uDEA8 Warning:** For this notebook to successfully run, you must have;\n",
    "* Trained and logged a model to the registry (e.g. `ml_model`)\n",
    "* Set the `catalog` and `schema` to point to your own. \n",
    "* Create pipeline parameters for input data path and model name (e.g. `mlpipeline.bronze_dataset_path`& `mlpipeline.model_name`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bce0820-9c9d-440b-9d35-ac9fdfe76991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Inference Pipeline\n",
    "\n",
    " MLflow-trained models can be used in Delta Live Tables pipelines. MLflow models are treated as transformations in Databricks, meaning they act upon a Spark DataFrame input and return results as a Spark DataFrame. Because Delta Live Tables defines datasets against DataFrames, you can convert Apache Spark workloads that leverage MLflow to Delta Live Tables with just a few lines of code.\n",
    "\n",
    "If you already have a Python notebook calling an MLflow model, you can adapt the code to Delta Live Tables by using the `@dlt.table` decorator and ensuring functions are defined to return transformation results. For an introduction to Delta Live Tables syntax, see Tutorial: [Declare a data pipeline with Python in Delta Live Tables.](https://docs.databricks.com/en/delta-live-tables/tutorial-python.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "007aa7fe-8d13-4d1f-9852-492ec0c6e15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Pipeline configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8893ffb-2099-426e-a46c-f36713631455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_dataset_path = spark.conf.get(\"mlpipeline.bronze_dataset_path\")\n",
    "model_name = spark.conf.get(\"mlpipeline.model_name\")\n",
    "alias_name = spark.conf.get(\"mlpipeline.model_alias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79417682-3b28-45c0-97e3-2ee3b41f42ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c04f9394-dd7c-4c1b-976b-2a698f52e4ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_uri=f\"models:/{model_name}@{alias_name}\" \n",
    "loaded_model_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark, \n",
    "    model_uri=model_uri, \n",
    "    result_type=\"string\",\n",
    "    env_manager = \"local\"\n",
    "    )\n",
    "\n",
    "primary_key = \"customerID\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b85c240e-d6b7-436c-9483-d846d919abe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f692176-7067-49ce-b78f-f85a54da8ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, struct\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"raw_inputs\",\n",
    "  comment=\"Raw inputs table\",\n",
    "  table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def raw_inputs():\n",
    "  return (\n",
    "    spark.read.csv(\n",
    "      bronze_dataset_path,\n",
    "      inferSchema=True,\n",
    "      header=True,\n",
    "      multiLine=True,\n",
    "      escape='\"'\n",
    "    )\n",
    "  )\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"features_input\",\n",
    "  comment=\"Features table\",\n",
    "  table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def features_input():\n",
    "  return (\n",
    "    dlt.read(\"raw_inputs\")\n",
    "      .select(primary_key, *features)\n",
    "      .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast(\"double\"))\n",
    "      .withColumn(\"tenure\", col(\"tenure\").cast(\"double\"))\n",
    "      .withColumn(\"TotalCharges\", F.expr(\"try_cast(trim(TotalCharges) as double)\"))\n",
    "      .na.drop(how=\"any\")\n",
    "  )\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"model_predictions\",\n",
    "  comment=\"Inference table\",\n",
    "  table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def model_predictions():\n",
    "  return (\n",
    "    dlt.read(\"features_input\")\n",
    "      # pack feature columns into a struct so the UDF receives a single argument\n",
    "      .withColumn(\"prediction\", loaded_model_udf(struct(*[col(c) for c in features])))\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "3.1b Demo - Inference Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}