{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8308684-3f0f-40c0-94c6-990add324124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b037bf-dc9b-4610-b553-66c19a33e35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Pipeline Deployment\n",
    "\n",
    "In this demo, we will show how to use a model as part of a data pipeline for inference. In the first section of the demo, we will prepare data and perform some basic feature engineering. Then, we will fit and register the model to model registry. Please note that these two steps are already covered in other courses and they are not the main focus of this demo. In the last section, which is the main focus of this demo, we will create a Lakeflow Declarative (FKA Delta Live Tables or DLT) pipeline and use the registered model as part of the pipeline. \n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* Describe steps for deploying a model within a pipeline.\n",
    "\n",
    "* Develop a simple pipeline that performs batch inference in its final step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33915578-3e7e-4a50-a7d9-5677ee115f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d777a9b-3e01-47fd-b8b5-0a687cc4b3a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35f3bf5c-a158-472f-a6bb-8128786c152a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dfe2702-8548-44d8-b7d4-4efad920ea33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-3.1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38ea2bda-9e55-4f84-b921-dbcc3ff78cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379a2c01-4cac-4129-8038-254606137189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc12e49-9734-43ce-b0b3-d84465164e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this demonstration, we will utilize a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including gender, as well as internet subscription details such as subscription plans and payment methods.\n",
    "\n",
    "After loading the dataset, we will perform simple **data cleaning and feature selection**. \n",
    "\n",
    "In the final step, we will split the dataset into **features** and **response** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6372b78-07db-4834-bc07-13462db37085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load dataset with spark\n",
    "shared_volume_name = 'telco' # From Marketplace\n",
    "csv_name = 'telco-customer-churn-missing' # CSV file name\n",
    "dataset_p_telco = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "\n",
    "# Dataset specs\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"] # Keeping numerical only for simplicity and demo purposes\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", F.expr(\"try_cast(trim(TotalCharges) as double)\"))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = telco_df.select(primary_key, *features)\n",
    "response_df = telco_df.select(primary_key, response)\n",
    "\n",
    "# Train a sklearn Decision Tree Classification model\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()\n",
    "\n",
    "for col in X_train_pdf.select_dtypes(\"int32\"):\n",
    "    X_train_pdf[col] = X_train_pdf[col].astype(\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7da56593-8d44-4c94-9826-da76b42dcb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(X_train_pdf.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb6630ef-28d2-4c3a-9893-45c4cced3135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Preparation\n",
    "\n",
    "**Note:** This section is not the main focus of this course. We are just repeating the model development and registration process here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6ee91d5-26ed-4048-becc-9b35ecc1cded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Setup Model Registry with UC\n",
    "\n",
    "Before we start model deployment, we need to fit and register a model. In this demo, **we will log models to Unity Catalog**, which means first we need to setup the **MLflow Model Registry URI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5631db44-943b-46b5-aec6-4a595356226f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b20a4b8-c2a4-4ab5-9977-f8b37c0f8146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fit and Register a Model with UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33631451-de17-4b8c-b3e1-c8a2a3585a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Use 3-level namespace for model name\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_model\" \n",
    "\n",
    "alias_name = \"pipeline\"\n",
    "\n",
    "# model to use for classification\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=10)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Deployment demo\") as mlflow_run:\n",
    "\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True)\n",
    "    \n",
    "    clf.fit(X_train_pdf, Y_train_pdf)\n",
    "\n",
    "    # Log model and push to registry\n",
    "    signature = infer_signature(X_train_pdf, Y_train_pdf)\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf,\n",
    "        artifact_path=\"decision_tree\",\n",
    "        signature=signature,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Set model alias\n",
    "    client.set_registered_model_alias(model_name, alias_name, get_latest_model_version(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1041bd0-7243-4a2e-a1bb-74297306871c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Pipeline to Run Batch Inference\n",
    "\n",
    "Now that our model is registered and ready, we can move on the most important part; using the model for inference inside a pipeline. \n",
    "\n",
    "**Note: The pipeline is already defined in `3.1b` notebook.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d329b8-c4d0-40f9-9c10-a3b530427a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Config Variables\n",
    "\n",
    "While defining the pipeline, you will need to use the following variables. Run the code block below first. Then, use the output in the next section while creating the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f795f6dc-9ff8-4c37-8f9d-210310f53369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"mlpipeline.bronze_dataset_path: {dataset_p_telco}\")\n",
    "print(f\"mlpipeline.model_name: {model_name}\")\n",
    "print(f\"mlpipeline.model_alias: {alias_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7adbf4a9-76ce-485d-bc02-e32bec769383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the ETL Pipeline\n",
    "This Vocareum environment has be configured so that **Lakeflow Declarative Pipelines** has been enabled (this feature is currently in Beta). \n",
    "> ****Note:**** To enable the Lakeflow Pipelines Editor: Open your user settings, go to Developer, and enable ****Lakeflow Pipelines Editor****.\n",
    "### Instructions\n",
    "1. Navigate to **Jobs & Pipelines** on the left side menu and click on **ETL pipeline** card at the top of the screen. \n",
    "2. Give the Pipeline the name `<labuserXXXXXXXX_XXXXXXXXXX>-pipeline`, where you need to replace `<labuserXXXXXXXX_XXXXXXXXXX>` with your labuser name.\n",
    "    - Click on the profile icon at the top right to copy your labuser name or see the output to cell 8 above.\n",
    "3. Make sure the catalog `dbacademy` is selected. \n",
    "4. Select your labuser schema, which is of the form `<labuserXXXXXXXX_XXXXXXXXXX>`. \n",
    "5. Select **Add existing assets** near the bottom under **Advanced options** \n",
    "</br>\n",
    "<img src=\"../Includes/Images/etl-pipeline-1.png\" width=\"500\"/>\n",
    "</br>\n",
    "6. In **Pipeline root folder**, locate and open the folder **M03 - Pipeline Deployment/Pipeline**. \n",
    "7. In **Source code paths**, click on the folder icon and select **3.1b Demo - Inference Pipeline** and click **Select**. \n",
    "8. Back in the **Add existing assets** select **Add** at the bottom right. \n",
    "9. Click on the **Pipeline** menu item at the top left and select the notebook **3.1b Demo - Inference Pipeline**. \n",
    "10. This new editor will display the notebook in the center of the screen and the **Pipeline graph** on the right of the screen. We will need to configure the variables shown in the notebook **3.1b Demo - Inference Pipeline** in the **Pipeline settings**. To do this, click on the **settings** icon next to **Pipeline configuration** to open the pipeline settings. Then, scroll down to the **Configuration** section and click **Add configuration** to set up the necessary variables for the pipeline.\n",
    "</br>\n",
    "<img src=\"../Includes/Images/add-config.png\" width=\"500\"/>\n",
    "</br>\n",
    "11. The config variable values are defined in the section **Config Variables** in this notebook (**3.1a Demo - Pipeline Deployment**). Copy and paste the key-value pair into the configuration and click **Save**.\n",
    "12. Back in **Pipeline settings**, navigate to and click **Dry run** at the top right. \n",
    "    - Dry-run mode allows you to test your policy configuration and monitor outbound connections without disrupting access to resources. This will not create or update any tables. \n",
    "13. Once the dry run is is completed, click **Run pipeline**. This will now create or update any tables in our pipeline. \n",
    "\n",
    "> Note we did not use classic compute for this pipeline run. We left **Serverless** as our compute by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66ceba3-760b-4a44-9964-38ff0553cbd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Resources and Trainings\n",
    "This demo is not a comprehensive introduction to **Lakeflow Declarative Pipelines**. For a deeper dive into this Databricks feature, check out our course **[Build Data Pipeline with Lakeflow Declarative Pipelines](https://www.databricks.com/training/catalog?search=lakeflow+declarative+pipelines)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db2c145b-7e3f-4c0c-a79e-dada33e80e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demonstration, we walked through the sequential process of training, registering, and deploying a model within a pipeline. Following the standard procedure of fitting and registering the model, we then established a Delta Live Tables pipeline. This pipeline not only ingests data from a source file but also implements necessary data transformations, culminating in the utilization of the registered model as the final step in the pipeline. While your specific project requirements may vary, this example illustrates how to set up and integrate a model for inference within the Delta Live Tables pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63407419-06f0-4bb4-b41b-aef4b98b3d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "3.1a Demo - Pipeline Deployment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}